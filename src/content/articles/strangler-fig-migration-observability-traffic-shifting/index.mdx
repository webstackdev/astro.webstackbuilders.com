---
title: "Strangler Fig Migration With Observability First"
description: "Incrementally migrating to new infrastructure by building observability before cutting traffic."
cover: "./cover.png"
coverAlt: "TODO"
author: "kevin-brown"
publishDate: 2024-01-15
tags: ["cloud-platforms"]
featured: true
---

*[CDN]: Content Delivery Network
*[DNS]: Domain Name System
*[ETL]: Extract, Transform, Load
*[MTTR]: Mean Time To Recovery
*[P50]: 50th Percentile (Median)
*[P99]: 99th Percentile
*[RPS]: Requests Per Second
*[SLI]: Service Level Indicator
*[SLO]: Service Level Objective
*[TTL]: Time To Live

Traffic shifting strategies, parallel running, comparison testing, and the metrics that tell you when migration is safe.

Migrate what you can observe; observe before you migrate.

## The Strangler Fig Pattern

Understanding the pattern before applying it to real migrations.

### Why Strangler Fig Works

The pattern succeeds because it eliminates the big-bang risk that kills most migrations.

```typescript
// Strangler fig vs big-bang comparison
interface MigrationApproach {
  name: string
  riskProfile: RiskProfile
  rollbackStrategy: string
  timeToValue: string
  observabilityRequirement: string
}

const migrationApproaches: MigrationApproach[] = [
  {
    name: 'Big-bang migration',
    riskProfile: {
      probability: 'High',
      impact: 'Catastrophic',
      blastRadius: '100% of traffic'
    },
    rollbackStrategy: 'Full rollback (if possible)',
    timeToValue: 'All at once at the end',
    observabilityRequirement: 'Post-migration only'
  },
  {
    name: 'Strangler fig',
    riskProfile: {
      probability: 'Low per increment',
      impact: 'Limited',
      blast radius: 'Only migrated traffic'
    },
    rollbackStrategy: 'Route traffic back to legacy',
    timeToValue: 'Incremental throughout',
    observabilityRequirement: 'Required at every step'
  }
]
```

```
Mermaid diagram: Strangler fig growth over time.
Phase 1: Legacy handles 100% of traffic
         New system handles 0%

Phase 2: Legacy handles 90% of traffic
         New system handles 10% (one endpoint)

Phase 3: Legacy handles 50% of traffic
         New system handles 50% (multiple endpoints)

Phase 4: Legacy handles 10% of traffic
         New system handles 90%

Phase 5: Legacy decommissioned
         New system handles 100%

Traffic shifts incrementally as confidence grows.
```

### Migration Unit Selection

Choosing what to migrate in what order.

```typescript
// Migration unit evaluation
interface MigrationCandidate {
  name: string
  type: 'endpoint' | 'feature' | 'domain' | 'service'
  metrics: {
    trafficVolume: number        // RPS
    businessCriticality: number  // 1-10
    complexity: number           // 1-10
    dependencies: number         // Count of upstream/downstream
    dataStoreAccess: string[]    // Which databases it touches
  }
  score: number
}

function scoreMigrationCandidate(candidate: MigrationCandidate): number {
  const {
    trafficVolume,
    businessCriticality,
    complexity,
    dependencies
  } = candidate.metrics

  // Ideal early candidates: low traffic, low criticality,
  // low complexity, few dependencies
  // Score higher = migrate later

  const trafficScore = Math.log10(trafficVolume + 1) * 10
  const criticalityScore = businessCriticality * 15
  const complexityScore = complexity * 10
  const dependencyScore = dependencies * 5

  return trafficScore + criticalityScore + complexityScore + dependencyScore
}

// Sort candidates by score (lowest first = migrate first)
function prioritizeMigrationOrder(
  candidates: MigrationCandidate[]
): MigrationCandidate[] {
  return [...candidates]
    .map(c => ({ ...c, score: scoreMigrationCandidate(c) }))
    .sort((a, b) => a.score - b.score)
}
```

| Migration Order | Characteristics | Risk Level | Learning Value |
|-----------------|-----------------|------------|----------------|
| First wave | Low traffic, low criticality, simple | Low | High (process learning) |
| Second wave | Medium traffic, moderate complexity | Medium | Medium (scaling learning) |
| Third wave | High traffic, high criticality | Higher | Lower (execution focus) |
| Last wave | Core business logic, high dependencies | Highest | N/A (must succeed) |

:::info[Start with Read-Only Endpoints]
Read-only endpoints (GET requests, queries) are ideal first migration candidates. They have no state modification risk, easy to compare responses, and can be rolled back instantly without data consistency concerns.
:::

### The Observability-First Principle

Building monitoring before migrating any traffic.

```typescript
// Observability requirements by migration phase
interface ObservabilityRequirements {
  phase: string
  mustHave: string[]
  niceToHave: string[]
  blockingCriteria: string
}

const observabilityByPhase: ObservabilityRequirements[] = [
  {
    phase: 'Pre-migration (legacy baseline)',
    mustHave: [
      'Request rate by endpoint',
      'Error rate by endpoint',
      'Latency percentiles (P50, P95, P99)',
      'Response payload samples',
      'Dependency call patterns'
    ],
    niceToHave: [
      'Request/response correlation IDs',
      'Business metric correlation'
    ],
    blockingCriteria: 'Cannot proceed without 2 weeks of baseline data'
  },
  {
    phase: 'Shadow traffic',
    mustHave: [
      'All legacy metrics on new system',
      'Response comparison results',
      'Latency differential',
      'Error differential'
    ],
    niceToHave: [
      'Automated diff analysis',
      'Anomaly detection on comparisons'
    ],
    blockingCriteria: 'Shadow traffic must run for 1 week minimum'
  },
  {
    phase: 'Traffic shifting',
    mustHave: [
      'Real-time traffic split visibility',
      'Instant error rate comparison',
      'Latency comparison dashboards',
      'Automatic rollback triggers'
    ],
    niceToHave: [
      'User-level traffic cohort tracking',
      'A/B test statistical significance'
    ],
    blockingCriteria: 'Dashboards must show split traffic clearly'
  }
]
```

## Building the Migration Baseline

Instrumenting the legacy system before touching traffic.

### Legacy System Instrumentation

Adding observability to systems that may have none.

```typescript
// Instrumentation wrapper for legacy endpoints
import { metrics, trace } from '@opentelemetry/api'

const meter = metrics.getMeter('legacy-migration')
const tracer = trace.getTracer('legacy-migration')

// Request metrics
const requestCounter = meter.createCounter('http_requests_total', {
  description: 'Total HTTP requests'
})

const requestDuration = meter.createHistogram('http_request_duration_ms', {
  description: 'Request duration in milliseconds',
  unit: 'ms'
})

const responseSize = meter.createHistogram('http_response_size_bytes', {
  description: 'Response body size in bytes',
  unit: 'bytes'
})

// Middleware to instrument all requests
function legacyInstrumentationMiddleware(
  req: Request,
  res: Response,
  next: NextFunction
) {
  const startTime = Date.now()
  const path = normalizePath(req.path)

  // Start trace span
  const span = tracer.startSpan(`${req.method} ${path}`, {
    attributes: {
      'http.method': req.method,
      'http.url': req.url,
      'http.route': path,
      'migration.system': 'legacy'
    }
  })

  // Capture response
  const originalSend = res.send
  res.send = function(body) {
    const duration = Date.now() - startTime
    const status = res.statusCode

    // Record metrics
    requestCounter.add(1, {
      method: req.method,
      path,
      status: String(status),
      system: 'legacy'
    })

    requestDuration.record(duration, {
      method: req.method,
      path,
      status: String(status)
    })

    if (body) {
      responseSize.record(Buffer.byteLength(body), {
        method: req.method,
        path
      })
    }

    // Complete span
    span.setAttributes({
      'http.status_code': status,
      'http.response_content_length': Buffer.byteLength(body || '')
    })
    span.end()

    return originalSend.call(this, body)
  }

  next()
}
```

### Capturing Response Signatures

Recording what legacy responses look like for comparison.

```typescript
// Response signature capture for comparison testing
interface ResponseSignature {
  requestId: string
  endpoint: string
  method: string
  requestHash: string  // Hash of normalized request
  responseHash: string // Hash of normalized response
  statusCode: number
  latencyMs: number
  timestamp: Date

  // For detailed comparison
  responseBody?: string
  responseHeaders?: Record<string, string>
}

class ResponseSignatureCapture {
  private store: SignatureStore
  private sampleRate: number

  constructor(store: SignatureStore, sampleRate = 0.01) {
    this.store = store
    this.sampleRate = sampleRate  // Sample 1% of traffic
  }

  captureMiddleware() {
    return (req: Request, res: Response, next: NextFunction) => {
      // Sample based on rate
      if (Math.random() > this.sampleRate) {
        return next()
      }

      const requestId = req.headers['x-request-id'] || generateId()
      const startTime = Date.now()

      // Capture request
      const requestHash = this.hashRequest(req)

      // Intercept response
      const chunks: Buffer[] = []
      const originalWrite = res.write
      const originalEnd = res.end

      res.write = function(chunk: any) {
        chunks.push(Buffer.from(chunk))
        return originalWrite.apply(res, arguments)
      }

      res.end = (chunk: any) => {
        if (chunk) chunks.push(Buffer.from(chunk))
        const body = Buffer.concat(chunks).toString('utf8')

        const signature: ResponseSignature = {
          requestId,
          endpoint: normalizePath(req.path),
          method: req.method,
          requestHash,
          responseHash: this.hashResponse(res.statusCode, body),
          statusCode: res.statusCode,
          latencyMs: Date.now() - startTime,
          timestamp: new Date(),
          responseBody: body,
          responseHeaders: res.getHeaders() as Record<string, string>
        }

        this.store.save(signature)

        return originalEnd.apply(res, arguments)
      }

      next()
    }
  }

  private hashRequest(req: Request): string {
    const normalized = {
      method: req.method,
      path: normalizePath(req.path),
      query: this.normalizeQuery(req.query),
      body: this.normalizeBody(req.body)
    }
    return createHash('sha256')
      .update(JSON.stringify(normalized))
      .digest('hex')
  }

  private hashResponse(status: number, body: string): string {
    const normalized = this.normalizeResponseBody(body)
    return createHash('sha256')
      .update(`${status}:${normalized}`)
      .digest('hex')
  }

  private normalizeResponseBody(body: string): string {
    try {
      const parsed = JSON.parse(body)
      // Remove volatile fields before hashing
      return JSON.stringify(this.removeVolatileFields(parsed))
    } catch {
      return body
    }
  }

  private removeVolatileFields(obj: any): any {
    const volatileKeys = ['timestamp', 'updatedAt', 'createdAt', 'requestId', 'traceId']
    if (Array.isArray(obj)) {
      return obj.map(item => this.removeVolatileFields(item))
    }
    if (obj && typeof obj === 'object') {
      return Object.fromEntries(
        Object.entries(obj)
          .filter(([key]) => !volatileKeys.includes(key))
          .map(([key, value]) => [key, this.removeVolatileFields(value)])
      )
    }
    return obj
  }
}
```

### Baseline Dashboard

Visualizing legacy behavior before migration.

```typescript
// Grafana dashboard configuration for migration baseline
const baselineDashboard = {
  title: 'Migration Baseline: [Service Name]',

  rows: [
    {
      title: 'Traffic Overview',
      panels: [
        {
          title: 'Request Rate by Endpoint',
          type: 'timeseries',
          query: `
            sum(rate(http_requests_total{system="legacy"}[5m])) by (path)
          `
        },
        {
          title: 'Error Rate by Endpoint',
          type: 'timeseries',
          query: `
            sum(rate(http_requests_total{system="legacy",status=~"5.."}[5m])) by (path)
            /
            sum(rate(http_requests_total{system="legacy"}[5m])) by (path)
          `
        }
      ]
    },
    {
      title: 'Latency Distribution',
      panels: [
        {
          title: 'P50 Latency by Endpoint',
          type: 'timeseries',
          query: `
            histogram_quantile(0.5,
              sum(rate(http_request_duration_ms_bucket{system="legacy"}[5m])) by (le, path)
            )
          `
        },
        {
          title: 'P99 Latency by Endpoint',
          type: 'timeseries',
          query: `
            histogram_quantile(0.99,
              sum(rate(http_request_duration_ms_bucket{system="legacy"}[5m])) by (le, path)
            )
          `
        }
      ]
    },
    {
      title: 'Response Characteristics',
      panels: [
        {
          title: 'Response Size Distribution',
          type: 'histogram',
          query: `
            sum(rate(http_response_size_bytes_bucket{system="legacy"}[5m])) by (le, path)
          `
        },
        {
          title: 'Status Code Distribution',
          type: 'piechart',
          query: `
            sum(increase(http_requests_total{system="legacy"}[24h])) by (status)
          `
        }
      ]
    }
  ]
}
```

## Shadow Traffic and Comparison Testing

Running traffic through both systems without serving from the new one.

### Shadow Traffic Architecture

Duplicating requests to the new system for comparison.

```
Mermaid diagram: Shadow traffic flow.
Client Request
      ↓
Load Balancer / Proxy
      ├── Primary path → Legacy System → Response to Client
      │
      └── Shadow path (async) → New System → Compare & Log
                                                ↓
                                          Comparison Results
                                          (not served to client)
```

```typescript
// Shadow traffic proxy implementation
import { createProxyMiddleware } from 'http-proxy-middleware'

interface ShadowConfig {
  legacyTarget: string
  newTarget: string
  shadowPercentage: number  // 0-100
  enabledEndpoints: string[]
}

function createShadowProxy(config: ShadowConfig) {
  const comparisonService = new ComparisonService()

  return async (req: Request, res: Response, next: NextFunction) => {
    const shouldShadow =
      config.enabledEndpoints.includes(normalizePath(req.path)) &&
      Math.random() * 100 < config.shadowPercentage

    // Always serve from legacy
    const legacyProxy = createProxyMiddleware({
      target: config.legacyTarget,
      changeOrigin: true,
      selfHandleResponse: true,
      onProxyRes: async (proxyRes, req, res) => {
        const legacyBody = await streamToBuffer(proxyRes)
        const legacyResponse = {
          status: proxyRes.statusCode,
          headers: proxyRes.headers,
          body: legacyBody.toString()
        }

        // Serve legacy response to client
        res.status(proxyRes.statusCode)
        Object.entries(proxyRes.headers).forEach(([k, v]) => {
          if (v) res.setHeader(k, v)
        })
        res.send(legacyBody)

        // Fire shadow request asynchronously (don't await)
        if (shouldShadow) {
          shadowRequest(req, legacyResponse, config, comparisonService)
        }
      }
    })

    legacyProxy(req, res, next)
  }
}

async function shadowRequest(
  originalReq: Request,
  legacyResponse: CapturedResponse,
  config: ShadowConfig,
  comparisonService: ComparisonService
) {
  const startTime = Date.now()

  try {
    // Replay request to new system
    const newResponse = await fetch(`${config.newTarget}${originalReq.url}`, {
      method: originalReq.method,
      headers: {
        ...originalReq.headers,
        'x-shadow-request': 'true',
        'x-original-request-id': originalReq.headers['x-request-id']
      },
      body: ['GET', 'HEAD'].includes(originalReq.method)
        ? undefined
        : JSON.stringify(originalReq.body)
    })

    const newBody = await newResponse.text()
    const newLatency = Date.now() - startTime

    // Compare responses
    await comparisonService.compare({
      requestId: originalReq.headers['x-request-id'],
      endpoint: normalizePath(originalReq.path),
      method: originalReq.method,
      legacy: {
        status: legacyResponse.status,
        body: legacyResponse.body,
        latencyMs: 0  // Not measured in this flow
      },
      new: {
        status: newResponse.status,
        body: newBody,
        latencyMs: newLatency
      }
    })
  } catch (error) {
    // Log shadow request failure but don't affect production
    comparisonService.recordShadowFailure({
      endpoint: originalReq.path,
      error: error.message
    })
  }
}
```

### Response Comparison Engine

Detecting differences between legacy and new system responses.

```typescript
// Response comparison with detailed diff analysis
interface ComparisonResult {
  requestId: string
  endpoint: string
  timestamp: Date
  match: boolean

  statusMatch: boolean
  bodyMatch: boolean
  latencyDelta: number  // new - legacy (positive = new is slower)

  differences?: Difference[]
}

interface Difference {
  path: string           // JSON path to difference
  type: 'missing' | 'extra' | 'changed' | 'type_mismatch'
  legacyValue?: any
  newValue?: any
}

class ComparisonService {
  private metricsRecorder: MetricsRecorder
  private diffStore: DiffStore

  async compare(comparison: ComparisonInput): Promise<ComparisonResult> {
    const statusMatch = comparison.legacy.status === comparison.new.status

    // Parse and normalize bodies
    let legacyParsed: any, newParsed: any
    let bodyMatch = false
    let differences: Difference[] = []

    try {
      legacyParsed = this.normalizeBody(JSON.parse(comparison.legacy.body))
      newParsed = this.normalizeBody(JSON.parse(comparison.new.body))

      differences = this.deepCompare(legacyParsed, newParsed, '')
      bodyMatch = differences.length === 0
    } catch {
      // Non-JSON bodies, compare as strings
      bodyMatch = comparison.legacy.body === comparison.new.body
      if (!bodyMatch) {
        differences = [{
          path: '$',
          type: 'changed',
          legacyValue: comparison.legacy.body.substring(0, 100),
          newValue: comparison.new.body.substring(0, 100)
        }]
      }
    }

    const result: ComparisonResult = {
      requestId: comparison.requestId,
      endpoint: comparison.endpoint,
      timestamp: new Date(),
      match: statusMatch && bodyMatch,
      statusMatch,
      bodyMatch,
      latencyDelta: comparison.new.latencyMs - comparison.legacy.latencyMs,
      differences: differences.length > 0 ? differences : undefined
    }

    // Record metrics
    this.recordMetrics(result)

    // Store mismatches for analysis
    if (!result.match) {
      await this.diffStore.save(result)
    }

    return result
  }

  private deepCompare(legacy: any, newVal: any, path: string): Difference[] {
    const differences: Difference[] = []

    if (typeof legacy !== typeof newVal) {
      differences.push({
        path,
        type: 'type_mismatch',
        legacyValue: typeof legacy,
        newValue: typeof newVal
      })
      return differences
    }

    if (Array.isArray(legacy)) {
      // Compare arrays element by element
      const maxLen = Math.max(legacy.length, newVal.length)
      for (let i = 0; i < maxLen; i++) {
        if (i >= legacy.length) {
          differences.push({
            path: `${path}[${i}]`,
            type: 'extra',
            newValue: newVal[i]
          })
        } else if (i >= newVal.length) {
          differences.push({
            path: `${path}[${i}]`,
            type: 'missing',
            legacyValue: legacy[i]
          })
        } else {
          differences.push(
            ...this.deepCompare(legacy[i], newVal[i], `${path}[${i}]`)
          )
        }
      }
    } else if (typeof legacy === 'object' && legacy !== null) {
      // Compare objects key by key
      const allKeys = new Set([...Object.keys(legacy), ...Object.keys(newVal)])

      for (const key of allKeys) {
        const newPath = path ? `${path}.${key}` : key

        if (!(key in legacy)) {
          differences.push({
            path: newPath,
            type: 'extra',
            newValue: newVal[key]
          })
        } else if (!(key in newVal)) {
          differences.push({
            path: newPath,
            type: 'missing',
            legacyValue: legacy[key]
          })
        } else {
          differences.push(
            ...this.deepCompare(legacy[key], newVal[key], newPath)
          )
        }
      }
    } else {
      // Primitive comparison
      if (legacy !== newVal) {
        differences.push({
          path,
          type: 'changed',
          legacyValue: legacy,
          newValue: newVal
        })
      }
    }

    return differences
  }

  private normalizeBody(body: any): any {
    // Remove fields that are expected to differ
    const ignoreFields = ['timestamp', 'requestId', 'traceId', 'serverTime']
    return this.removeFields(body, ignoreFields)
  }

  private recordMetrics(result: ComparisonResult) {
    // Match rate
    this.metricsRecorder.increment('shadow_comparisons_total', {
      endpoint: result.endpoint,
      match: String(result.match)
    })

    // Latency delta histogram
    this.metricsRecorder.histogram('shadow_latency_delta_ms', result.latencyDelta, {
      endpoint: result.endpoint
    })

    // Status match rate
    this.metricsRecorder.increment('shadow_status_match_total', {
      endpoint: result.endpoint,
      match: String(result.statusMatch)
    })
  }
}
```

### Shadow Traffic Dashboard

Monitoring comparison results to build confidence.

```typescript
// Grafana dashboard for shadow traffic analysis
const shadowDashboard = {
  title: 'Migration Shadow Traffic Analysis',

  rows: [
    {
      title: 'Overall Match Rate',
      panels: [
        {
          title: 'Response Match Rate',
          type: 'stat',
          query: `
            sum(rate(shadow_comparisons_total{match="true"}[1h]))
            /
            sum(rate(shadow_comparisons_total[1h]))
          `,
          thresholds: {
            green: 0.99,   // 99%+ match rate
            yellow: 0.95,  // 95-99% match rate
            red: 0         // Below 95%
          }
        },
        {
          title: 'Match Rate by Endpoint',
          type: 'table',
          query: `
            sum(rate(shadow_comparisons_total{match="true"}[24h])) by (endpoint)
            /
            sum(rate(shadow_comparisons_total[24h])) by (endpoint)
          `
        }
      ]
    },
    {
      title: 'Latency Comparison',
      panels: [
        {
          title: 'Latency Delta Distribution',
          type: 'histogram',
          query: `
            sum(rate(shadow_latency_delta_ms_bucket[1h])) by (le)
          `,
          annotation: 'Negative = new is faster, Positive = new is slower'
        },
        {
          title: 'P99 Latency Delta by Endpoint',
          type: 'timeseries',
          query: `
            histogram_quantile(0.99,
              sum(rate(shadow_latency_delta_ms_bucket[5m])) by (le, endpoint)
            )
          `
        }
      ]
    },
    {
      title: 'Mismatch Analysis',
      panels: [
        {
          title: 'Mismatch Types',
          type: 'piechart',
          query: `
            sum(increase(shadow_mismatch_types_total[24h])) by (type)
          `
        },
        {
          title: 'Top Mismatching Fields',
          type: 'table',
          query: `
            topk(10, sum(increase(shadow_diff_path_total[24h])) by (path))
          `
        }
      ]
    }
  ]
}
```

:::warning[Shadow Traffic Gotchas]
Shadow traffic works for read operations but is dangerous for writes. Duplicating POST/PUT/DELETE requests can cause double-writes, duplicate charges, or data corruption. Use request tagging or synthetic data for write operation testing.
:::

## Traffic Shifting Strategies

Moving real traffic from legacy to new system.

### Percentage-Based Traffic Split

Gradually increasing new system traffic.

```typescript
// Traffic splitting configuration
interface TrafficSplitConfig {
  endpoint: string
  legacyWeight: number  // 0-100
  newWeight: number     // 0-100 (legacyWeight + newWeight = 100)
  stickySession: boolean
  rollbackThreshold: RollbackThreshold
}

interface RollbackThreshold {
  errorRateIncrease: number      // e.g., 0.01 = 1% increase triggers rollback
  latencyP99Increase: number     // e.g., 1.5 = 50% increase triggers rollback
  evaluationWindow: Duration
}

// Example: Progressive traffic shift schedule
const trafficShiftSchedule = [
  { day: 0, newWeight: 1, gate: 'Manual verification of first requests' },
  { day: 1, newWeight: 5, gate: '24 hours at 1% with <0.1% error rate increase' },
  { day: 3, newWeight: 10, gate: '48 hours at 5% with latency within 10%' },
  { day: 5, newWeight: 25, gate: '48 hours at 10% with no incidents' },
  { day: 7, newWeight: 50, gate: '48 hours at 25% with no incidents' },
  { day: 10, newWeight: 75, gate: '72 hours at 50% with no incidents' },
  { day: 14, newWeight: 100, gate: '96 hours at 75% with no incidents' }
]
```

```yaml
# Istio traffic splitting configuration
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: api-migration
spec:
  hosts:
    - api.example.com
  http:
    - match:
        - uri:
            prefix: /api/users  # Migrated endpoint
      route:
        - destination:
            host: legacy-api
            port:
              number: 80
          weight: 90
        - destination:
            host: new-api
            port:
              number: 80
          weight: 10

    # Non-migrated endpoints go to legacy only
    - route:
        - destination:
            host: legacy-api
            port:
              number: 80
          weight: 100
```

### Cohort-Based Migration

Migrating specific user groups rather than random traffic.

```typescript
// Cohort-based traffic routing
interface CohortConfig {
  cohortId: string
  description: string
  selectionCriteria: SelectionCriteria
  targetSystem: 'legacy' | 'new'
}

interface SelectionCriteria {
  type: 'user_id_hash' | 'user_attribute' | 'organization' | 'geographic'
  parameters: Record<string, any>
}

const cohortMigrationStrategy: CohortConfig[] = [
  {
    cohortId: 'internal_users',
    description: 'Internal employees for early testing',
    selectionCriteria: {
      type: 'user_attribute',
      parameters: { attribute: 'email_domain', value: 'company.com' }
    },
    targetSystem: 'new'
  },
  {
    cohortId: 'beta_users',
    description: 'Users who opted into beta',
    selectionCriteria: {
      type: 'user_attribute',
      parameters: { attribute: 'beta_enabled', value: true }
    },
    targetSystem: 'new'
  },
  {
    cohortId: 'low_risk_orgs',
    description: 'Small organizations with low criticality',
    selectionCriteria: {
      type: 'organization',
      parameters: {
        tier: ['free', 'starter'],
        maxSeats: 10
      }
    },
    targetSystem: 'new'
  },
  {
    cohortId: 'geographic_rollout',
    description: 'Users in specific regions',
    selectionCriteria: {
      type: 'geographic',
      parameters: { regions: ['eu-west-1', 'us-west-2'] }
    },
    targetSystem: 'new'
  }
]

// Router implementation
class CohortRouter {
  private cohorts: CohortConfig[]
  private defaultTarget: 'legacy' | 'new'

  constructor(cohorts: CohortConfig[], defaultTarget: 'legacy' | 'new' = 'legacy') {
    this.cohorts = cohorts
    this.defaultTarget = defaultTarget
  }

  routeRequest(request: Request, user: User): 'legacy' | 'new' {
    for (const cohort of this.cohorts) {
      if (this.matchesCriteria(user, cohort.selectionCriteria)) {
        return cohort.targetSystem
      }
    }
    return this.defaultTarget
  }

  private matchesCriteria(user: User, criteria: SelectionCriteria): boolean {
    switch (criteria.type) {
      case 'user_id_hash':
        // Consistent hashing for deterministic routing
        const hash = createHash('md5').update(user.id).digest('hex')
        const bucket = parseInt(hash.substring(0, 8), 16) % 100
        return bucket < criteria.parameters.percentage

      case 'user_attribute':
        return user[criteria.parameters.attribute] === criteria.parameters.value

      case 'organization':
        return criteria.parameters.tier.includes(user.organization.tier) &&
               user.organization.seatCount <= criteria.parameters.maxSeats

      case 'geographic':
        return criteria.parameters.regions.includes(user.region)

      default:
        return false
    }
  }
}
```

```
Mermaid diagram: Cohort-based migration progression.
Week 1: Internal employees (100 users) → New System
        Everyone else → Legacy

Week 2: + Beta users (1,000 users) → New System

Week 3: + Free tier orgs (10,000 users) → New System

Week 4: + Starter tier orgs (50,000 users) → New System

Week 5: + EU region (100,000 users) → New System

Week 6: + US West region → New System

Week 8: All traffic → New System

Each cohort provides feedback before expanding.
```

### Automatic Rollback

Detecting problems and reverting traffic automatically.

```typescript
// Automatic rollback controller
interface RollbackController {
  serviceName: string
  metrics: RollbackMetrics
  config: RollbackConfig
}

interface RollbackMetrics {
  errorRateBaseline: number
  latencyP99Baseline: number
  currentErrorRate: number
  currentLatencyP99: number
}

interface RollbackConfig {
  errorRateThreshold: number    // Absolute error rate to trigger rollback
  errorRateIncrease: number     // Relative increase from baseline
  latencyP99Threshold: number   // Absolute latency to trigger rollback
  latencyP99Multiplier: number  // e.g., 2 = trigger if 2x baseline
  evaluationWindow: Duration
  cooldownPeriod: Duration
}

class AutomaticRollbackController {
  private config: RollbackConfig
  private lastRollbackTime: Date | null = null

  async evaluate(metrics: RollbackMetrics): Promise<RollbackDecision> {
    // Check cooldown
    if (this.lastRollbackTime &&
        Date.now() - this.lastRollbackTime.getTime() < this.config.cooldownPeriod.ms) {
      return { action: 'wait', reason: 'In cooldown period' }
    }

    // Check error rate
    if (metrics.currentErrorRate > this.config.errorRateThreshold) {
      return {
        action: 'rollback',
        reason: `Error rate ${(metrics.currentErrorRate * 100).toFixed(2)}% exceeds threshold ${(this.config.errorRateThreshold * 100).toFixed(2)}%`,
        severity: 'critical'
      }
    }

    const errorRateIncrease = metrics.currentErrorRate - metrics.errorRateBaseline
    if (errorRateIncrease > this.config.errorRateIncrease) {
      return {
        action: 'rollback',
        reason: `Error rate increased by ${(errorRateIncrease * 100).toFixed(2)}% (threshold: ${(this.config.errorRateIncrease * 100).toFixed(2)}%)`,
        severity: 'high'
      }
    }

    // Check latency
    if (metrics.currentLatencyP99 > this.config.latencyP99Threshold) {
      return {
        action: 'rollback',
        reason: `P99 latency ${metrics.currentLatencyP99}ms exceeds threshold ${this.config.latencyP99Threshold}ms`,
        severity: 'critical'
      }
    }

    const latencyMultiplier = metrics.currentLatencyP99 / metrics.latencyP99Baseline
    if (latencyMultiplier > this.config.latencyP99Multiplier) {
      return {
        action: 'rollback',
        reason: `P99 latency ${latencyMultiplier.toFixed(1)}x baseline (threshold: ${this.config.latencyP99Multiplier}x)`,
        severity: 'high'
      }
    }

    return { action: 'continue', reason: 'Metrics within acceptable range' }
  }

  async executeRollback(currentSplit: TrafficSplit): Promise<TrafficSplit> {
    this.lastRollbackTime = new Date()

    // Roll back to previous safe state
    const newSplit: TrafficSplit = {
      ...currentSplit,
      legacyWeight: 100,
      newWeight: 0
    }

    // Alert on-call
    await this.alert({
      severity: 'critical',
      title: `Automatic rollback triggered for ${currentSplit.endpoint}`,
      description: `Traffic shifted back to legacy due to metric degradation`
    })

    return newSplit
  }
}
```

```yaml
# Prometheus alerting rules for migration rollback
groups:
  - name: migration-rollback-alerts
    rules:
      - alert: MigrationErrorRateHigh
        expr: |
          (
            sum(rate(http_requests_total{system="new",status=~"5.."}[5m])) by (endpoint)
            /
            sum(rate(http_requests_total{system="new"}[5m])) by (endpoint)
          ) > 0.01
        for: 2m
        labels:
          severity: critical
          action: rollback
        annotations:
          summary: "New system error rate exceeds 1% for {{ $labels.endpoint }}"

      - alert: MigrationLatencyRegression
        expr: |
          histogram_quantile(0.99, sum(rate(http_request_duration_ms_bucket{system="new"}[5m])) by (le, endpoint))
          >
          histogram_quantile(0.99, sum(rate(http_request_duration_ms_bucket{system="legacy"}[5m])) by (le, endpoint)) * 1.5
        for: 5m
        labels:
          severity: warning
          action: investigate
        annotations:
          summary: "New system P99 latency 50%+ higher than legacy for {{ $labels.endpoint }}"
```

## Migration Completion and Legacy Decommissioning

Finishing the migration and retiring the old system.

### Completion Criteria

Defining when migration is truly complete.

```typescript
// Migration completion checklist
interface CompletionCriteria {
  category: string
  criteria: Criterion[]
}

const completionChecklist: CompletionCriteria[] = [
  {
    category: 'Traffic',
    criteria: [
      { name: '100% traffic on new system', check: () => getNewSystemTrafficPercent() === 100 },
      { name: 'No traffic to legacy for 7+ days', check: () => getLegacyLastRequestAge() > days(7) },
      { name: 'All endpoints migrated', check: () => getUnmigratedEndpoints().length === 0 }
    ]
  },
  {
    category: 'Stability',
    criteria: [
      { name: '7 days at 100% with no rollbacks', check: () => getFullTrafficDuration() > days(7) },
      { name: 'Error rate within baseline', check: () => getErrorRateDelta() < 0.001 },
      { name: 'Latency within 10% of baseline', check: () => getLatencyDelta() < 0.10 }
    ]
  },
  {
    category: 'Observability',
    criteria: [
      { name: 'New system dashboards complete', check: () => dashboardsExist() },
      { name: 'Alerting configured', check: () => alertsConfigured() },
      { name: 'On-call runbooks updated', check: () => runbooksUpdated() }
    ]
  },
  {
    category: 'Data',
    criteria: [
      { name: 'Data migration verified', check: () => dataMigrationComplete() },
      { name: 'No legacy database dependencies', check: () => noLegacyDbConnections() },
      { name: 'Data retention policies updated', check: () => retentionPoliciesUpdated() }
    ]
  },
  {
    category: 'Organizational',
    criteria: [
      { name: 'Team trained on new system', check: () => teamTrainingComplete() },
      { name: 'Documentation updated', check: () => docsUpdated() },
      { name: 'Stakeholder sign-off obtained', check: () => signOffObtained() }
    ]
  }
]
```

### Legacy System Sunset

Safely decommissioning the old system.

```typescript
// Legacy sunset timeline
const sunsetTimeline = [
  {
    phase: 'Traffic removal',
    duration: 'Already complete',
    actions: [
      'Route 100% traffic to new system',
      'Keep legacy running but receiving no traffic'
    ]
  },
  {
    phase: 'Monitoring period',
    duration: '2 weeks',
    actions: [
      'Monitor for any unexpected legacy traffic',
      'Check for hardcoded references to legacy endpoints',
      'Verify no background jobs still calling legacy'
    ]
  },
  {
    phase: 'Read-only mode',
    duration: '1 week',
    actions: [
      'Disable write operations on legacy',
      'Return 503 for POST/PUT/DELETE with helpful message',
      'Log any requests that reach legacy'
    ]
  },
  {
    phase: 'Scream test',
    duration: '1 week',
    actions: [
      'Stop legacy service',
      'Monitor for any errors in dependent systems',
      'Keep infrastructure in place for quick restart'
    ]
  },
  {
    phase: 'Decommission',
    duration: '1 day',
    actions: [
      'Remove legacy infrastructure',
      'Archive legacy codebase',
      'Update DNS records',
      'Delete legacy databases (after backup verification)'
    ]
  }
]
```

```
Mermaid diagram: Legacy decommissioning state machine.
[100% New Traffic]
       ↓
Monitoring Period (2 weeks)
   - Any legacy traffic? → Investigate and fix
       ↓
Read-Only Mode (1 week)
   - Requests to legacy? → Log and redirect
       ↓
Scream Test (1 week)
   - Errors from shutdown? → Restart and investigate
       ↓
Decommission
   - Remove infrastructure
   - Archive code
   - Delete data (after backup)
       ↓
[Migration Complete]
```

### Post-Migration Retrospective

Capturing lessons learned for future migrations.

```typescript
// Migration retrospective template
interface MigrationRetrospective {
  migrationName: string
  duration: {
    planned: Duration
    actual: Duration
    delta: Duration
  }

  metrics: {
    endpointsMigrated: number
    rollbackCount: number
    incidentsDuringMigration: number
    totalDowntime: Duration
  }

  whatWentWell: string[]
  whatCouldImprove: string[]
  unexpectedChallenges: string[]

  recommendations: Recommendation[]
}

const retrospectiveTemplate: MigrationRetrospective = {
  migrationName: '[Service] Legacy to [New Service]',
  duration: {
    planned: { weeks: 8 },
    actual: { weeks: 12 },
    delta: { weeks: 4 }  // Over by 4 weeks
  },

  metrics: {
    endpointsMigrated: 45,
    rollbackCount: 3,
    incidentsDuringMigration: 1,
    totalDowntime: { minutes: 15 }
  },

  whatWentWell: [
    'Baseline instrumentation caught issues before users noticed',
    'Shadow traffic identified 15 response differences early',
    'Automatic rollback prevented two potential incidents',
    'Cohort-based migration allowed quick feedback loops'
  ],

  whatCouldImprove: [
    'Should have started baseline collection 4 weeks earlier',
    'Write operation testing was insufficient',
    'Team training happened too late in the process',
    'Underestimated data migration complexity'
  ],

  unexpectedChallenges: [
    'Legacy system had undocumented caching behavior',
    'Some clients had hardcoded IP addresses',
    'Database schema differences required ETL pipeline',
    'Third-party webhook endpoints needed coordination'
  ],

  recommendations: [
    {
      category: 'Process',
      recommendation: 'Start baseline collection 4+ weeks before migration begins',
      priority: 'High'
    },
    {
      category: 'Technical',
      recommendation: 'Build synthetic traffic generator for write operations',
      priority: 'High'
    },
    {
      category: 'Organizational',
      recommendation: 'Include API consumer teams in migration planning',
      priority: 'Medium'
    }
  ]
}
```

:::success[Migration Success Indicators]
A successful strangler fig migration has these characteristics: zero extended outages, fewer than 3 rollbacks, less than 50% schedule overrun, and the team says they'd use the same approach again.
:::

## Conclusion

Strangler fig migrations succeed because they trade big-bang risk for incremental progress. The key is observability-first: instrument the legacy system thoroughly before migrating any traffic, so you have a baseline to compare against. Shadow traffic reveals response differences without user impact—run it for at least a week before shifting real traffic. Use percentage-based splits for broad migrations and cohort-based routing when you need targeted feedback from specific user groups. Automatic rollback protects users when metrics degrade—configure it to trigger before users notice problems. Complete the migration only when all criteria are met: 100% traffic on the new system, stable metrics for 7+ days, and organizational readiness. Decommission the legacy system gradually through monitoring periods and scream tests. Document lessons learned—every migration teaches something that makes the next one smoother.

---

## Cover Image Prompts

### Prompt 1: Strangler Fig Tree on Ancient Ruins
Photograph of a strangler fig tree wrapped around ancient temple ruins (like Ta Prohm in Cambodia). Massive roots engulfing stone architecture. Dappled sunlight through the canopy. The organic taking over the structural—the perfect metaphor for the pattern's namesake.

### Prompt 2: Traffic Flow Split Visualization
Abstract visualization of traffic flow splitting like a river delta or arterial system. Bright streams dividing from one into two paths of different sizes. Dark background with luminous blue/green flow lines. Data visualization aesthetic showing the percentage split.

### Prompt 3: Bridge Construction with Traffic
Photograph of a highway bridge being rebuilt alongside an existing bridge with traffic still flowing. The old and new structures parallel to each other. Construction equipment visible. The reality of maintaining service while building the replacement.

### Prompt 4: Dual Dashboard Comparison
Photograph or render of a control room with two large monitors side by side showing identical dashboard layouts with different metrics. One labeled "Legacy" one labeled "New". Operators studying the comparison. The visual comparison that drives migration confidence.

### Prompt 5: Migration Timeline Visualization
Infographic-style illustration showing a horizontal timeline with gradual color transition from orange (legacy) to blue (new). Key milestones marked along the timeline. Clean minimal design. The progressive nature of strangler fig migration over time.
