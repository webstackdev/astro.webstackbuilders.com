---
title: "SLOs for Internal Services Nobody Cares About"
description: "Introducing reliability culture to teams that have never measured availability or set targets."
cover: "./cover.png"
coverAlt: "TODO"
author: "kevin-brown"
publishDate: 2024-01-15
tags: ["reliability-and-testing"]
featured: true
---

*[API]: Application Programming Interface
*[CI]: Continuous Integration
*[MTTR]: Mean Time To Recovery
*[OKR]: Objectives and Key Results
*[P99]: 99th Percentile
*[SLA]: Service Level Agreement
*[SLI]: Service Level Indicator
*[SLO]: Service Level Objective
*[SRE]: Site Reliability Engineering
*[TOIL]: Manual, repetitive, automatable work

Starting point selection, stakeholder buy-in, incremental adoption, and proving value before expanding scope.

Start with what teams already complain about.

## Why Internal Services Resist SLOs

The cultural and structural barriers that make SLO adoption hard for teams that have never measured reliability.

### The "It Works for Us" Problem

Internal services often operate without SLOs because their consumers have no leverage and failures stay invisible.

```typescript
// Why internal services avoid reliability commitments
interface InternalServiceDynamics {
  noExternalPressure: {
    situation: 'No paying customers = no contractual SLAs',
    result: 'No forcing function for reliability investment',
    manifestation: 'Outages handled when convenient, not urgently'
  }

  captiveAudience: {
    situation: 'Internal consumers can\'t switch to competitors',
    result: 'No incentive to improve experience',
    manifestation: '"Just retry" becomes acceptable guidance'
  }

  invisibleFailures: {
    situation: 'Failures blamed on downstream consumer',
    result: 'No accountability for reliability issues',
    manifestation: '"Works on my machine" extends to services'
  }

  successTheater: {
    situation: 'Green dashboards showing infrastructure metrics',
    result: 'False confidence in service health',
    manifestation: 'CPU at 30% while requests timeout'
  }
}
```

:::warning[The Hidden Cost of "Good Enough"]
Internal services without SLOs create hidden tax on every team that depends on them. Each consumer builds defensive code, retry logic, and workarounds. The total cost across all consumers often exceeds what reliability investment would cost.
:::

### Common Objections and Responses

Addressing the resistance you'll encounter when proposing SLOs.

| Objection | What They Mean | How to Respond |
|-----------|---------------|----------------|
| "We don't have time for this" | SLOs seem like extra work | SLOs reduce fire-fighting time |
| "Our service is simple" | Reliability seems like overkill | Simple services still have consumers waiting |
| "Nobody has complained" | No complaints = no problem | Consumers have given up complaining |
| "We'd need to hire SREs" | SLOs require special skills | Basic SLOs need basic math, not specialists |
| "Management won't care" | No executive sponsorship | Frame as developer productivity, not reliability |

```typescript
// Reframing SLOs for resistant teams
interface SLOReframing {
  fromReliabilityTalk: string
  toDeveloperProductivity: string
  evidence: string
}

const reframingExamples: SLOReframing[] = [
  {
    fromReliabilityTalk: 'We need 99.9% availability',
    toDeveloperProductivity: 'Engineers spend 8 hours/week debugging integration issues',
    evidence: 'Survey consuming teams about time spent on integration problems'
  },
  {
    fromReliabilityTalk: 'We should measure error rates',
    toDeveloperProductivity: 'Deployments fail unpredictably because dependencies are unstable',
    evidence: 'Track deployment success rate correlated with dependency health'
  },
  {
    fromReliabilityTalk: 'We need latency SLOs',
    toDeveloperProductivity: 'CI pipelines are slow because internal tools timeout randomly',
    evidence: 'Measure CI duration variance caused by flaky internal services'
  }
]
```

### The Measurement Chicken-and-Egg

Teams without observability can't set SLOs, but won't invest in observability without SLOs to justify it.

```
Mermaid diagram: The measurement chicken-and-egg cycle.

"We can't set SLOs"
      ↓
"Because we don't have metrics"
      ↓
"We don't have budget for observability"
      ↓
"Because there's no reliability mandate"
      ↓
"There's no mandate because we can't prove unreliability"
      ↓
(Back to "We can't set SLOs")

Breaking the cycle:
- Start with what you CAN measure (logs, existing metrics)
- Use consumer complaints as proxy for SLI violations
- Bootstrap with minimal instrumentation
```

## Finding Your First SLO Candidate

Selecting the right service for initial SLO adoption.

### Service Selection Criteria

Characteristics that make a service a good first SLO candidate.

```typescript
// Scoring potential SLO adoption candidates
interface SLOCandidateScore {
  service: string
  scores: {
    visibility: number      // 1-5: How visible are failures?
    pain: number           // 1-5: How much do consumers complain?
    simplicity: number     // 1-5: How easy to define SLIs?
    measurability: number  // 1-5: Can we measure it today?
    teamReceptivity: number // 1-5: Will the team participate?
    impactPotential: number // 1-5: Will success influence others?
  }
  totalScore: number
  recommendation: string
}

function scoreCandidate(service: ServiceInfo): SLOCandidateScore {
  const scores = {
    // Highly visible failures are easier to prove SLO value
    visibility: service.hasIncidentHistory ? 5 :
                service.consumersComplain ? 4 :
                service.hasMonitoring ? 3 : 1,

    // Pain creates motivation
    pain: service.consumerSurveyScore < 3 ? 5 :
          service.incidentsPerMonth > 2 ? 4 :
          service.escalationsPerMonth > 0 ? 3 : 1,

    // Simple services have clear SLIs
    simplicity: service.endpointCount < 5 ? 5 :
                service.endpointCount < 15 ? 4 :
                service.endpointCount < 30 ? 3 : 2,

    // Can we measure without major investment?
    measurability: service.hasPrometheus ? 5 :
                   service.hasStructuredLogs ? 4 :
                   service.hasAnyLogs ? 3 : 1,

    // Team attitude matters
    teamReceptivity: service.teamAskedForHelp ? 5 :
                     service.teamIsNeutral ? 3 :
                     service.teamIsResistant ? 1 : 2,

    // Success here should inspire others
    impactPotential: service.consumerCount > 10 ? 5 :
                     service.isOnCriticalPath ? 5 :
                     service.consumerCount > 5 ? 3 : 2
  }

  const totalScore = Object.values(scores).reduce((a, b) => a + b, 0)

  return {
    service: service.name,
    scores,
    totalScore,
    recommendation: totalScore >= 25 ? 'Start here' :
                    totalScore >= 18 ? 'Good second candidate' :
                    'Wait for better conditions'
  }
}
```

### Avoid Starting with the Hardest Cases

Services that seem most important might be worst first candidates.

```typescript
// Services to avoid for first SLO adoption
const badFirstCandidates = [
  {
    type: 'Legacy monolith',
    problems: [
      'Too many failure modes to define SLIs',
      'No clear ownership for reliability',
      'Success here won\'t feel replicable'
    ],
    whenToTackle: 'After simpler wins establish credibility'
  },
  {
    type: 'Hostile team ownership',
    problems: [
      'Will fight every decision',
      'Will declare failure at first setback',
      'Will badmouth initiative to others'
    ],
    whenToTackle: 'After success stories change the narrative'
  },
  {
    type: 'Perfect reliability already',
    problems: [
      'No pain to motivate adoption',
      'SLOs will just document status quo',
      'Won\'t demonstrate value of SLO practice'
    ],
    whenToTackle: 'Include later for completeness'
  },
  {
    type: 'External dependency dominated',
    problems: [
      'Can\'t control primary failure sources',
      'SLOs will reflect vendor reliability',
      'Team will feel powerless'
    ],
    whenToTackle: 'After learning to define SLOs for controllable services'
  }
]
```

:::info[The Goldilocks Candidate]
The ideal first candidate has visible problems (so success is demonstrable), motivated team (so adoption is collaborative), and simple enough to define clear SLIs (so scope doesn't explode). It should be broken enough to fix but not so broken that fixing seems impossible.
:::

### Consumer Pain Discovery

Finding reliability problems by asking consumers, not providers.

```typescript
// Consumer survey template
interface ConsumerSurvey {
  serviceName: string
  questions: SurveyQuestion[]
}

const consumerSurveyTemplate: SurveyQuestion[] = [
  {
    question: 'In the past month, how often has [Service] caused problems for your work?',
    type: 'frequency',
    options: ['Never', 'Once or twice', 'Weekly', 'Multiple times per week', 'Daily']
  },
  {
    question: 'What types of problems have you experienced?',
    type: 'multiselect',
    options: [
      'Service unavailable/returning errors',
      'Slow responses affecting my service',
      'Inconsistent behavior (works sometimes)',
      'Data quality issues',
      'Unexpected breaking changes'
    ]
  },
  {
    question: 'How do you currently handle [Service] reliability issues?',
    type: 'multiselect',
    options: [
      'Retry logic in my code',
      'Circuit breaker pattern',
      'Fall back to cached data',
      'Manual intervention when it breaks',
      'Wait and hope it recovers',
      'Escalate to the team'
    ]
  },
  {
    question: 'How much time does your team spend per month dealing with [Service] issues?',
    type: 'estimate',
    units: 'hours'
  },
  {
    question: 'What reliability level would meet your needs?',
    type: 'selection',
    options: [
      'Always available (99.99%+)',
      'Rarely down (99.9%)',
      'Occasional issues acceptable (99%)',
      'Don\'t know/doesn\'t matter'
    ]
  }
]

// Aggregate survey results into SLO justification
function aggregateSurveyResults(
  responses: SurveyResponse[]
): SLOJustification {
  const totalHoursWasted = responses.reduce(
    (sum, r) => sum + r.timeSpentOnIssues, 0
  )

  const consumersExperiencingWeeklyIssues = responses.filter(
    r => ['Weekly', 'Multiple times per week', 'Daily'].includes(r.problemFrequency)
  ).length

  const reliabilityExpectation = responses
    .map(r => r.reliabilityNeeded)
    .sort()[Math.floor(responses.length / 2)]  // Median

  return {
    consumerCount: responses.length,
    monthlyHoursWasted: totalHoursWasted,
    estimatedMonthlyCost: totalHoursWasted * ENGINEERING_HOURLY_RATE,
    percentWithWeeklyIssues: consumersExperiencingWeeklyIssues / responses.length * 100,
    medianReliabilityExpectation: reliabilityExpectation,
    narrative: generateJustificationNarrative(/*...*/)
  }
}
```

## Bootstrapping Minimal Observability

Getting enough measurement in place to set initial SLOs.

### Starting with What Exists

Using existing logs and metrics before investing in new tooling.

```typescript
// Observability bootstrapping from existing sources
interface ObservabilityBootstrap {
  source: string
  whatItProvides: string[]
  limitations: string[]
  slisPossible: string[]
}

const bootstrapSources: ObservabilityBootstrap[] = [
  {
    source: 'Access logs (nginx, Apache)',
    whatItProvides: [
      'Request count by status code',
      'Response time (if logged)',
      'Endpoint breakdown'
    ],
    limitations: [
      'No application-level detail',
      'May need log parsing setup',
      'Historical data limited by retention'
    ],
    slisPossible: ['Availability (status codes)', 'Latency (if logged)']
  },
  {
    source: 'Application logs',
    whatItProvides: [
      'Error messages and stack traces',
      'Business logic failures',
      'User-facing error counts'
    ],
    limitations: [
      'Unstructured logs hard to aggregate',
      'May not log successful requests',
      'Inconsistent logging across endpoints'
    ],
    slisPossible: ['Error rate (by parsing ERROR level)', 'Specific failure modes']
  },
  {
    source: 'Database slow query log',
    whatItProvides: [
      'Queries exceeding threshold',
      'Query patterns causing problems',
      'Database-level latency'
    ],
    limitations: [
      'Only shows slow queries, not normal ones',
      'Doesn\'t connect to user impact',
      'Threshold may be wrong'
    ],
    slisPossible: ['Database latency contribution']
  },
  {
    source: 'Cloud provider metrics',
    whatItProvides: [
      'Request counts and error rates',
      'Latency percentiles',
      'Resource utilization'
    ],
    limitations: [
      'May not reflect user experience',
      'Limited to what cloud exposes',
      'Can be expensive at high cardinality'
    ],
    slisPossible: ['Availability', 'Latency', 'Throughput']
  }
]

// Quick log parsing for SLI bootstrapping
const logParsingExamples = {
  // Nginx access log availability
  availability: `
    # Count requests by status code from nginx logs
    cat access.log | \\
      awk '{print $9}' | \\
      sort | uniq -c | sort -rn

    # Result:
    # 45000 200
    # 3000 404
    # 500 500
    # 100 503

    # Availability = (45000 + 3000) / 48600 = 98.8%
  `,

  // Response time extraction
  latency: `
    # If response time is logged in last field
    cat access.log | \\
      awk '{print $NF}' | \\
      sort -n | \\
      awk 'BEGIN{c=0} {a[c++]=$1} END{print "P50:", a[int(c*0.5)], "P99:", a[int(c*0.99)]}'
  `
}
```

### Minimal Instrumentation Addition

Adding just enough instrumentation to support basic SLOs.

```typescript
// Express middleware for minimal SLI instrumentation
import { Registry, Counter, Histogram } from 'prom-client'

const register = new Registry()

// Minimal metrics for SLOs
const requestsTotal = new Counter({
  name: 'http_requests_total',
  help: 'Total HTTP requests',
  labelNames: ['method', 'path', 'status'],
  registers: [register]
})

const requestDuration = new Histogram({
  name: 'http_request_duration_seconds',
  help: 'HTTP request duration in seconds',
  labelNames: ['method', 'path'],
  buckets: [0.01, 0.05, 0.1, 0.25, 0.5, 1, 2.5, 5, 10],
  registers: [register]
})

// Middleware that adds ~1ms overhead
function sliMiddleware(req: Request, res: Response, next: NextFunction) {
  const start = process.hrtime.bigint()

  res.on('finish', () => {
    const durationNs = process.hrtime.bigint() - start
    const durationSec = Number(durationNs) / 1e9

    // Normalize path to avoid cardinality explosion
    const normalizedPath = normalizePath(req.path)

    requestsTotal.inc({
      method: req.method,
      path: normalizedPath,
      status: res.statusCode
    })

    requestDuration.observe(
      { method: req.method, path: normalizedPath },
      durationSec
    )
  })

  next()
}

// Path normalization to control label cardinality
function normalizePath(path: string): string {
  // Replace UUIDs with placeholder
  path = path.replace(
    /[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}/gi,
    ':id'
  )
  // Replace numeric IDs with placeholder
  path = path.replace(/\/\d+/g, '/:id')
  return path
}

// Expose metrics endpoint
app.get('/metrics', async (req, res) => {
  res.set('Content-Type', register.contentType)
  res.send(await register.metrics())
})
```

```
Mermaid diagram: Minimal observability architecture.
Application
  ├── SLI Middleware (adds metrics)
  └── /metrics endpoint
         ↓
Prometheus (scrapes every 15s)
         ↓
Grafana Dashboard
  ├── Availability SLI
  ├── Latency SLI
  └── Error Budget remaining
```

### Grafana Dashboard Template

A minimal dashboard to track SLOs before investing in SLO tooling.

```json
{
  "title": "[Service] SLO Dashboard",
  "panels": [
    {
      "title": "Availability (Current)",
      "type": "stat",
      "targets": [{
        "expr": "sum(rate(http_requests_total{status=~\"2..\"}[5m])) / sum(rate(http_requests_total[5m]))"
      }],
      "fieldConfig": {
        "defaults": {
          "unit": "percentunit",
          "thresholds": {
            "steps": [
              { "value": 0.999, "color": "green" },
              { "value": 0.99, "color": "yellow" },
              { "value": 0, "color": "red" }
            ]
          }
        }
      }
    },
    {
      "title": "Availability (7d rolling)",
      "type": "timeseries",
      "targets": [{
        "expr": "sum(increase(http_requests_total{status=~\"2..\"}[1h])) / sum(increase(http_requests_total[1h]))",
        "legendFormat": "Availability"
      }]
    },
    {
      "title": "Latency P99",
      "type": "stat",
      "targets": [{
        "expr": "histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))"
      }],
      "fieldConfig": {
        "defaults": {
          "unit": "s"
        }
      }
    },
    {
      "title": "Error Budget Remaining (30d)",
      "type": "gauge",
      "targets": [{
        "expr": "(0.001 - (1 - sum(increase(http_requests_total{status=~\"2..\"}[30d])) / sum(increase(http_requests_total[30d])))) / 0.001 * 100"
      }],
      "description": "Percentage of 99.9% error budget remaining",
      "fieldConfig": {
        "defaults": {
          "unit": "percent",
          "min": 0,
          "max": 100,
          "thresholds": {
            "steps": [
              { "value": 50, "color": "green" },
              { "value": 20, "color": "yellow" },
              { "value": 0, "color": "red" }
            ]
          }
        }
      }
    }
  ]
}
```

## Setting Initial SLO Targets

Choosing targets that are achievable, meaningful, and non-controversial.

### The "Current State Minus" Approach

Setting first SLOs slightly below current performance to guarantee initial success.

```typescript
// Initial SLO target calculation
interface InitialSLOCalculation {
  metric: string
  currentPerformance: number
  suggestedTarget: number
  rationale: string
}

function calculateInitialSLO(
  metricName: string,
  historicalData: number[],
  worstAcceptable: number
): InitialSLOCalculation {
  // Calculate percentiles of historical performance
  const sorted = [...historicalData].sort((a, b) => a - b)
  const p5 = sorted[Math.floor(sorted.length * 0.05)]   // 5th percentile (bad days)
  const p50 = sorted[Math.floor(sorted.length * 0.5)]   // Median
  const p95 = sorted[Math.floor(sorted.length * 0.95)]  // 95th percentile

  // Initial target should be:
  // - Achievable: below current median
  // - Meaningful: above worst acceptable
  // - Defensible: based on actual data

  const suggestedTarget = Math.max(
    p5 * 0.95,          // Slightly below worst historical
    worstAcceptable     // But not below business minimum
  )

  return {
    metric: metricName,
    currentPerformance: p50,
    suggestedTarget,
    rationale: `
      Current median: ${p50.toFixed(3)}
      Worst 5%: ${p5.toFixed(3)}
      Suggested target: ${suggestedTarget.toFixed(3)}
      This target will be met ~95% of the time based on history,
      leaving headroom for initial adoption without pressure.
    `
  }
}

// Example calculation
const availabilityHistory = [
  0.998, 0.999, 0.995, 0.999, 0.998, 0.997, 0.999, 0.996,  // 8 weeks
  0.999, 0.998, 0.999, 0.992, 0.998, 0.999, 0.997, 0.998   // 8 more weeks
]

const initialSLO = calculateInitialSLO('availability', availabilityHistory, 0.99)
// Result:
// currentPerformance: 0.998
// suggestedTarget: 0.992 * 0.95 = 0.9424 → capped at worstAcceptable 0.99
// Initial SLO: 99%
```

### Avoiding Analysis Paralysis

Moving forward with imperfect targets rather than debating forever.

```typescript
// Decision framework for contentious SLO discussions
interface SLODecisionFramework {
  situation: string
  defaultDecision: string
  revisitIn: string
}

const sloDecisionDefaults: SLODecisionFramework[] = [
  {
    situation: 'Can\'t agree on latency threshold',
    defaultDecision: 'Use 2x current P99 as initial target',
    revisitIn: '30 days with actual data'
  },
  {
    situation: 'Don\'t know what availability target to set',
    defaultDecision: 'Start with 99% (7.3 hours/month budget)',
    revisitIn: '30 days, tighten if easily met'
  },
  {
    situation: 'Stakeholders want different targets',
    defaultDecision: 'Use the most achievable (least aggressive)',
    revisitIn: 'Quarterly review with stakeholder data'
  },
  {
    situation: 'Not sure which endpoints to include',
    defaultDecision: 'Include all non-health-check endpoints',
    revisitIn: 'After first incident, refine scope'
  },
  {
    situation: 'Team says target is too aggressive',
    defaultDecision: 'Lower it until team agrees',
    revisitIn: 'Monthly, with data on actual performance'
  }
]
```

:::info[Perfect Is the Enemy of Adopted]
An imperfect SLO that the team uses is infinitely more valuable than a perfect SLO that never gets implemented. Start with something achievable, iterate based on data.
:::

### SLO Proposal Document

Template for proposing an SLO to a team and their stakeholders.

```markdown
# SLO Proposal: [Service Name]

## Summary
We propose establishing availability and latency SLOs for [Service]
to formalize reliability expectations and enable data-driven decisions
about reliability investment.

## Business Context
- **Consumers**: [List of teams/services that depend on this]
- **Business impact of outages**: [What breaks when this breaks]
- **Current state**: [No SLOs / informal expectations / ad-hoc targets]

## Proposed SLOs

### Availability SLO
- **Definition**: Proportion of requests returning 2xx or expected 4xx
- **Target**: 99.5% over 30-day rolling window
- **Error budget**: 3.6 hours/month of allowed downtime
- **Measurement**: [Prometheus / CloudWatch / logs]

### Latency SLO
- **Definition**: Proportion of requests completing under threshold
- **Target**: P99 < 500ms for 99% of 5-minute windows
- **Measurement**: [Source of latency data]

## Justification
Based on [X weeks] of historical data:
- Current availability: [Y%]
- Current P99 latency: [Z ms]
- Proposed targets are achievable with current system

Consumer survey (N=[X] responses):
- [Y%] experience weekly reliability issues
- [Z] hours/month spent on workarounds
- Median reliability expectation: [A%]

## Error Budget Policy
When error budget is exhausted:
1. Freeze non-critical changes to [Service]
2. Focus engineering effort on reliability improvements
3. Conduct review to identify root causes

## Success Metrics
After 90 days, we will evaluate:
- SLO achievement rate
- Reduction in consumer complaints
- Reduction in time spent on reliability issues
- Team adoption of error budget mindset

## Timeline
- Week 1: Deploy instrumentation
- Week 2: Establish baseline
- Week 3: Finalize targets with stakeholders
- Week 4: Activate SLO tracking and alerting
- Week 8: First review

## Risks and Mitigations
| Risk | Mitigation |
|------|------------|
| Team resists adoption | Start with achievable targets |
| Metrics are noisy | Tune measurement before setting targets |
| Targets too aggressive | Review and adjust after 30 days |
```

## Building Stakeholder Buy-In

Getting leadership and teams to support SLO adoption.

### Speaking Management's Language

Translating reliability into business metrics executives care about.

```typescript
// Translating SLOs to business metrics
interface BusinessTranslation {
  sloMetric: string
  businessTranslation: string
  executiveHook: string
  dataNeeded: string[]
}

const businessTranslations: BusinessTranslation[] = [
  {
    sloMetric: 'Availability',
    businessTranslation: 'Developer hours lost to outages',
    executiveHook: 'Every 1% of downtime costs X developer-hours per month',
    dataNeeded: [
      'Number of consumers',
      'Average engineers per consuming team',
      'Hours per incident per team'
    ]
  },
  {
    sloMetric: 'Latency P99',
    businessTranslation: 'CI pipeline time and velocity',
    executiveHook: 'Slow internal services add Y minutes to every deploy',
    dataNeeded: [
      'CI jobs calling this service per day',
      'Latency distribution of service calls in CI',
      'Cost per CI minute'
    ]
  },
  {
    sloMetric: 'Error rate',
    businessTranslation: 'Customer escalations and support load',
    executiveHook: 'Internal service errors surface as customer complaints',
    dataNeeded: [
      'Support tickets mentioning related features',
      'Time to resolve escalations',
      'Customer churn correlated with reliability'
    ]
  }
]

// Cost calculation template
function calculateReliabilityCost(
  serviceMetrics: ServiceMetrics,
  consumerInfo: ConsumerInfo
): BusinessCase {
  const {
    averageOutagesPerMonth,
    averageOutageDuration,
    averageLatencyP99
  } = serviceMetrics

  const {
    consumingTeams,
    engineersPerTeam,
    hourlyRate,
    ciJobsPerDay
  } = consumerInfo

  // Developer time lost to outages
  const hoursLostToOutages =
    averageOutagesPerMonth *
    consumingTeams *
    2 *  // Engineers actively impacted per outage
    averageOutageDuration

  const outagesCost = hoursLostToOutages * hourlyRate

  // CI time lost to latency
  const ciMinutesLostPerDay =
    ciJobsPerDay *
    (averageLatencyP99 / 60) *
    0.1  // Fraction of jobs calling this service

  const ciCostPerMonth = ciMinutesLostPerDay * 30 * CI_COST_PER_MINUTE

  return {
    monthlyOutageCost: outagesCost,
    monthlyCICost: ciCostPerMonth,
    totalMonthlyCost: outagesCost + ciCostPerMonth,
    narrative: `Internal service reliability issues cost approximately
      $${(outagesCost + ciCostPerMonth).toFixed(0)}/month in developer
      productivity. SLO implementation investment: ~$${SLO_IMPLEMENTATION_COST}.
      Payback period: ${SLO_IMPLEMENTATION_COST / (outagesCost + ciCostPerMonth)} months.`
  }
}
```

### Getting Team Ownership

Helping teams see SLOs as their tool, not external mandate.

```typescript
// Team engagement strategies
interface TeamEngagementStrategy {
  teamState: string
  approach: string
  tactics: string[]
  antiPatterns: string[]
}

const engagementStrategies: TeamEngagementStrategy[] = [
  {
    teamState: 'Skeptical but open',
    approach: 'Collaborative exploration',
    tactics: [
      'Ask what they\'d like to know about their service reliability',
      'Offer to help instrument, not mandate metrics',
      'Let them choose initial SLI definitions',
      'Celebrate early data insights, not compliance'
    ],
    antiPatterns: [
      'Imposing metrics from outside',
      'Setting targets without consultation',
      'Emphasizing accountability over insight'
    ]
  },
  {
    teamState: 'Overwhelmed with other priorities',
    approach: 'Minimal viable SLO',
    tactics: [
      'Do the instrumentation work for them',
      'Start with single, simple SLI',
      'Make dashboards before asking for commitment',
      'Show value with existing data before asking for investment'
    ],
    antiPatterns: [
      'Adding to their workload',
      'Requiring their active participation initially',
      'Complex multi-SLI proposals'
    ]
  },
  {
    teamState: 'Already reliability-conscious',
    approach: 'Formalization and amplification',
    tactics: [
      'Recognize what they\'re already doing well',
      'Help formalize informal SLOs they already track',
      'Connect them to broader initiatives',
      'Ask them to mentor other teams'
    ],
    antiPatterns: [
      'Treating them like beginners',
      'Ignoring their existing practices',
      'Imposing new frameworks that conflict with theirs'
    ]
  }
]
```

### The Pilot Program Approach

Structuring initial adoption as a time-bounded experiment.

```typescript
// Pilot program structure
interface SLOPilotProgram {
  name: string
  duration: Duration
  participants: PilotParticipant[]
  successCriteria: Criterion[]
  escalationPath: string
  exitConditions: ExitCondition[]
}

const pilotTemplate: SLOPilotProgram = {
  name: 'SLO Pilot: Q1 2024',
  duration: { weeks: 12 },

  participants: [
    {
      service: '[Primary candidate service]',
      team: '[Team name]',
      sponsor: '[Engineering manager]',
      commitment: 'Set SLOs, track for 8 weeks, retrospective'
    },
    {
      service: '[Secondary candidate]',
      team: '[Team name]',
      sponsor: '[Engineering manager]',
      commitment: 'Observer status, may join mid-pilot'
    }
  ],

  successCriteria: [
    {
      metric: 'SLOs defined and tracked',
      target: '100% of pilot services',
      measurement: 'SLO documentation and dashboards exist'
    },
    {
      metric: 'Team satisfaction',
      target: '> 3.5/5 on usefulness rating',
      measurement: 'End-of-pilot survey'
    },
    {
      metric: 'Consumer satisfaction',
      target: 'Improvement from baseline',
      measurement: 'Pre/post consumer survey'
    },
    {
      metric: 'Incident correlation',
      target: 'SLO violations correlate with consumer impact',
      measurement: 'Post-incident analysis'
    }
  ],

  escalationPath: 'Pilot concerns escalate to [Sponsor Name]',

  exitConditions: [
    {
      condition: 'Success',
      criteria: 'All success criteria met',
      nextStep: 'Expand to additional services'
    },
    {
      condition: 'Partial success',
      criteria: 'Some criteria met, learnings valuable',
      nextStep: 'Iterate and run second pilot'
    },
    {
      condition: 'Failure',
      criteria: 'Team satisfaction < 2/5 OR no value demonstrated',
      nextStep: 'Retrospective, decide whether to continue'
    }
  ]
}
```

## Operating with SLOs

Making SLOs part of daily team operations.

### Error Budget Policies

Defining what happens when error budget runs low or exhausts.

```typescript
// Error budget policy framework
interface ErrorBudgetPolicy {
  budgetLevel: string
  threshold: string
  actions: string[]
  decisionMaker: string
}

const errorBudgetPolicies: ErrorBudgetPolicy[] = [
  {
    budgetLevel: 'Healthy',
    threshold: '> 50% remaining',
    actions: [
      'Normal development velocity',
      'Feature work prioritized',
      'Reliability improvements opportunistic'
    ],
    decisionMaker: 'Team leads'
  },
  {
    budgetLevel: 'Caution',
    threshold: '20-50% remaining',
    actions: [
      'Review recent reliability incidents',
      'Prioritize reliability backlog items',
      'Increase risky change scrutiny',
      'Consider slowing release velocity'
    ],
    decisionMaker: 'Team leads with manager awareness'
  },
  {
    budgetLevel: 'At risk',
    threshold: '5-20% remaining',
    actions: [
      'Pause non-critical feature work',
      'Focus on reliability improvements',
      'Require extra review for all changes',
      'Daily budget monitoring'
    ],
    decisionMaker: 'Engineering manager'
  },
  {
    budgetLevel: 'Exhausted',
    threshold: '< 5% or negative',
    actions: [
      'Freeze all non-emergency changes',
      'Dedicated reliability sprint',
      'Incident review for all budget-consuming events',
      'Stakeholder communication about reliability focus'
    ],
    decisionMaker: 'Director level'
  }
]

// Automated error budget tracking
interface ErrorBudgetTracker {
  service: string
  slo: SLO
  budgetConsumedPercent: number
  burnRate: number  // Budget consumption per day
  projectedExhaustion: Date | null
  currentPolicy: ErrorBudgetPolicy
}

function calculateBudgetStatus(
  slo: SLO,
  performanceData: PerformanceData[]
): ErrorBudgetTracker {
  const windowDays = 30
  const budgetTotal = (1 - slo.target) * windowDays * 24 * 60  // minutes

  const badMinutes = performanceData
    .filter(d => d.value < slo.target)
    .reduce((sum, d) => sum + d.duration, 0)

  const budgetConsumedPercent = (badMinutes / budgetTotal) * 100
  const remainingPercent = 100 - budgetConsumedPercent

  // Calculate burn rate (budget consumption per day)
  const daysElapsed = daysBetween(performanceData[0].timestamp, new Date())
  const burnRate = budgetConsumedPercent / daysElapsed

  // Project when budget will exhaust
  const daysRemaining = remainingPercent / burnRate
  const projectedExhaustion = burnRate > 0
    ? addDays(new Date(), daysRemaining)
    : null

  return {
    service: slo.service,
    slo,
    budgetConsumedPercent,
    burnRate,
    projectedExhaustion,
    currentPolicy: determinePolicyLevel(remainingPercent)
  }
}
```

### SLO-Driven Prioritization

Using error budgets to make trade-off decisions.

```
Mermaid diagram: SLO-driven prioritization flowchart.

Start: New feature request
  ↓
Is error budget healthy (>50%)?
  ├── Yes → Evaluate feature normally
  │           ↓
  │         Does feature add reliability risk?
  │           ├── No → Proceed with feature
  │           └── Yes → Mitigate risk first, then proceed
  │
  └── No → Is feature business-critical?
              ├── No → Defer until budget recovers
              └── Yes → Can it ship with no reliability risk?
                          ├── Yes → Proceed carefully
                          └── No → Escalate to director
```

```typescript
// Feature prioritization with SLO context
interface FeaturePrioritization {
  feature: string
  businessValue: number       // 1-10
  reliabilityRisk: number     // 1-10 (10 = high risk)
  currentBudgetHealth: number // % remaining
  decision: 'proceed' | 'defer' | 'escalate'
  rationale: string
}

function prioritizeFeature(
  feature: FeatureRequest,
  budgetStatus: ErrorBudgetTracker
): FeaturePrioritization {
  const businessValue = assessBusinessValue(feature)
  const reliabilityRisk = assessReliabilityRisk(feature)
  const budgetHealth = 100 - budgetStatus.budgetConsumedPercent

  let decision: 'proceed' | 'defer' | 'escalate'
  let rationale: string

  if (budgetHealth > 50) {
    // Healthy budget - normal prioritization
    decision = 'proceed'
    rationale = 'Error budget healthy, proceed with normal risk assessment'
  } else if (budgetHealth > 20) {
    // Caution zone
    if (reliabilityRisk <= 3) {
      decision = 'proceed'
      rationale = 'Low-risk feature acceptable in caution zone'
    } else {
      decision = 'defer'
      rationale = 'Higher-risk features deferred until budget recovers'
    }
  } else if (budgetHealth > 5) {
    // At-risk zone
    if (businessValue >= 8 && reliabilityRisk <= 2) {
      decision = 'escalate'
      rationale = 'High-value low-risk, escalate for manager decision'
    } else {
      decision = 'defer'
      rationale = 'Budget at risk, deferring non-critical features'
    }
  } else {
    // Exhausted
    if (businessValue === 10) {  // Business-critical
      decision = 'escalate'
      rationale = 'Critical feature needs director approval'
    } else {
      decision = 'defer'
      rationale = 'Budget exhausted, focus on reliability'
    }
  }

  return { feature: feature.name, businessValue, reliabilityRisk,
           currentBudgetHealth: budgetHealth, decision, rationale }
}
```

### Regular SLO Reviews

Establishing cadence for reviewing and adjusting SLOs.

```typescript
// SLO review meeting template
interface SLOReviewAgenda {
  cadence: string
  duration: string
  attendees: string[]
  sections: AgendaSection[]
}

const weeklyReviewAgenda: SLOReviewAgenda = {
  cadence: 'Weekly',
  duration: '30 minutes',
  attendees: ['Service owner', 'On-call engineer', 'SRE representative'],
  sections: [
    {
      topic: 'SLO performance summary',
      duration: '5 min',
      content: 'Review dashboard: current SLI values, budget consumption',
      presenter: 'On-call engineer'
    },
    {
      topic: 'Budget-consuming events',
      duration: '10 min',
      content: 'Discuss any incidents or degradations that consumed budget',
      presenter: 'On-call engineer'
    },
    {
      topic: 'Upcoming risks',
      duration: '5 min',
      content: 'Planned changes that might impact reliability',
      presenter: 'Service owner'
    },
    {
      topic: 'Action items',
      duration: '10 min',
      content: 'Reliability improvements to prioritize this week',
      presenter: 'All'
    }
  ]
}

const quarterlyReviewAgenda: SLOReviewAgenda = {
  cadence: 'Quarterly',
  duration: '60 minutes',
  attendees: ['Service owner', 'Engineering manager', 'Major consumers', 'SRE'],
  sections: [
    {
      topic: 'SLO achievement summary',
      duration: '10 min',
      content: 'Review quarter performance vs targets'
    },
    {
      topic: 'Consumer satisfaction',
      duration: '15 min',
      content: 'Survey results from consuming teams'
    },
    {
      topic: 'SLO adjustment proposals',
      duration: '20 min',
      content: 'Propose tightening or loosening targets with justification'
    },
    {
      topic: 'Next quarter reliability priorities',
      duration: '15 min',
      content: 'Agree on reliability investments for next quarter'
    }
  ]
}
```

## Expanding Adoption

Growing from pilot success to organization-wide SLO culture.

### The Adoption Flywheel

Creating momentum that makes SLO adoption self-sustaining.

```
Mermaid diagram: SLO adoption flywheel.

Success stories from early adopters
         ↓
Other teams ask "how do we do that?"
         ↓
Tooling and templates make adoption easier
         ↓
More teams adopt SLOs
         ↓
Cross-team visibility improves
         ↓
Dependency management gets easier
         ↓
Leadership sees value, provides support
         ↓
(Back to Success stories)

Flywheel accelerators:
- Internal presentations on wins
- Shared dashboards and templates
- SLO-aware tooling integration
```

### Internal Marketing

Promoting SLO success to encourage organic adoption.

```typescript
// Success story template
interface SLOSuccessStory {
  service: string
  team: string
  duration: string
  highlights: {
    before: string
    after: string
    businessImpact: string
  }
  quote: {
    text: string
    author: string
    role: string
  }
  lessonsLearned: string[]
}

const successStoryExample: SLOSuccessStory = {
  service: 'Authentication Service',
  team: 'Identity Platform',
  duration: '6 months with SLOs',

  highlights: {
    before: '3-4 unplanned outages per month affecting downstream teams',
    after: 'Zero outages in last 3 months, 99.95% availability maintained',
    businessImpact: 'Reduced authentication-related support tickets by 60%'
  },

  quote: {
    text: 'SLOs gave us a shared language with our consumers. Instead of arguing about whether the service was "good enough," we could point to data.',
    author: 'Sarah Chen',
    role: 'Tech Lead, Identity Platform'
  },

  lessonsLearned: [
    'Started with modest 99.5% target, tightened to 99.9% after 3 months',
    'Consumer survey revealed issues we didn\'t know about',
    'Error budget policy prevented two risky deploys that would have caused outages'
  ]
}

// Communication channels for success stories
const promotionChannels = [
  {
    channel: 'All-hands presentation',
    frequency: 'Quarterly',
    content: '5-minute segment on SLO wins',
    audience: 'Engineering organization'
  },
  {
    channel: 'Engineering newsletter',
    frequency: 'Monthly',
    content: 'Feature story on team adoption',
    audience: 'All engineers'
  },
  {
    channel: 'Slack channel',
    frequency: 'Ongoing',
    content: 'Quick wins and dashboard screenshots',
    audience: 'Interested parties'
  },
  {
    channel: 'Demo day',
    frequency: 'Per cohort',
    content: 'Teams present their SLO journey',
    audience: 'Potential adopters'
  }
]
```

### Scaling Support

Building infrastructure to support many teams adopting SLOs.

```typescript
// SLO platform capabilities for scale
interface SLOPlatform {
  capabilities: PlatformCapability[]
  selfService: boolean
  governance: GovernanceModel
}

const sloPlatformRoadmap: SLOPlatform = {
  capabilities: [
    {
      name: 'SLO registry',
      description: 'Central catalog of all SLOs across organization',
      value: 'Discoverability and consistency',
      phase: 1
    },
    {
      name: 'Dashboard templates',
      description: 'Pre-built Grafana dashboards for common SLI patterns',
      value: 'Reduce time-to-first-SLO',
      phase: 1
    },
    {
      name: 'Automated SLI collection',
      description: 'Common instrumentation library for all services',
      value: 'Consistent measurement',
      phase: 2
    },
    {
      name: 'Error budget tracking',
      description: 'Automated budget calculation and alerting',
      value: 'Proactive budget management',
      phase: 2
    },
    {
      name: 'SLO-as-code',
      description: 'YAML definitions in repos, CI validation',
      value: 'Version control and review',
      phase: 3
    },
    {
      name: 'Dependency SLO rollup',
      description: 'Composite SLOs showing end-to-end reliability',
      value: 'Cross-team visibility',
      phase: 3
    }
  ],

  selfService: true,  // Teams can onboard without central team help

  governance: {
    whoSetsTargets: 'Service teams with consumer input',
    whoApproves: 'Team manager + SRE review',
    minimumCoverage: 'All tier-1 services must have SLOs',
    reviewCadence: 'Quarterly'
  }
}
```

### Measuring Adoption Success

Tracking whether SLO adoption is achieving its goals.

```typescript
// Adoption success metrics
interface AdoptionMetrics {
  coverage: CoverageMetrics
  effectiveness: EffectivenessMetrics
  culture: CultureMetrics
}

const adoptionDashboard: AdoptionMetrics = {
  coverage: {
    servicesWithSLOs: {
      target: '100% of tier-1, 80% of tier-2',
      current: 85,
      unit: 'percent'
    },
    teamsUsingErrorBudgets: {
      target: '> 70%',
      current: 60,
      unit: 'percent'
    },
    slosDocumented: {
      target: '100%',
      current: 95,
      unit: 'percent'
    }
  },

  effectiveness: {
    slosMet: {
      description: 'Percentage of SLOs meeting target',
      current: 92,
      target: '>90%',
      notes: 'Too high might mean targets too easy'
    },
    incidentsCorrelatedWithSLO: {
      description: 'Incidents where SLO violation was detected',
      current: 78,
      target: '>80%',
      notes: 'Low correlation means SLOs missing real issues'
    },
    meanTimeToDetection: {
      description: 'How quickly SLO violations are noticed',
      current: 5,
      target: '<10',
      unit: 'minutes'
    }
  },

  culture: {
    teamSatisfaction: {
      description: 'Teams rating SLOs as useful',
      current: 4.1,
      target: '>3.5',
      scale: '1-5'
    },
    errorBudgetDecisions: {
      description: 'Features deferred or prioritized based on budget',
      current: 12,
      period: 'month',
      notes: 'Shows SLOs influencing real decisions'
    },
    sloMentionsInReviews: {
      description: 'SLOs referenced in design docs and PRs',
      current: 'increasing',
      notes: 'Cultural integration indicator'
    }
  }
}
```

:::success[Signs of Cultural Adoption]
You know SLOs have become part of the culture when teams reference error budget in prioritization discussions, when SLOs appear in design documents unprompted, and when teams proactively tighten targets rather than waiting for mandates.
:::

## Conclusion

SLO adoption for internal services requires solving a chicken-and-egg problem: teams resist measurement because they've never had reliability expectations, but you can't set expectations without measurement. Break the cycle by starting with consumer pain discovery, bootstrapping minimal observability from existing logs, and setting achievable targets based on current performance. Choose pilot candidates carefully—services with visible problems, motivated teams, and simple enough scope to define clear SLIs. Speak management's language by translating reliability into developer productivity and business cost. Build team ownership by making SLOs their tool for understanding, not an external mandate for compliance. Operate with error budget policies that give SLOs teeth—when budget runs low, feature work pauses. Expand adoption through success stories and shared tooling that makes onboarding easy. The goal isn't SLO coverage for its own sake—it's a reliability culture where teams understand their service health, consumers have clear expectations, and trade-off decisions are made with data instead of politics.

---

## Cover Image Prompts

### Prompt 1: Seeds Growing in Office Environment
Photograph of small plants sprouting from soil in an office setting—perhaps in a meeting room or at a desk. Natural light from windows. The contrast between organic growth and corporate environment. Metaphor for cultivating culture.

### Prompt 2: Bridge Building in Progress
Architectural photograph of a bridge under construction, showing scaffolding and structural elements coming together. Morning golden hour light. The sense of connecting two sides, building infrastructure that enables connection.

### Prompt 3: Domino Effect Setup
Overhead photograph of dominoes arranged in an elaborate pattern, some already fallen, chain reaction in progress. High contrast lighting on a dark surface. The idea of initial action creating cascading adoption.

### Prompt 4: Team Huddle with Shared Dashboard
Photograph of diverse team members gathered around a large monitor displaying graphs and metrics. Engaged expressions, collaborative pointing at data. Warm office lighting. The shared understanding that data provides.

### Prompt 5: Compass on Engineering Blueprint
Close-up photograph of a brass compass resting on technical blueprints or architectural drawings. Shallow depth of field with compass in sharp focus. The metaphor of finding direction through measurement and standards.
