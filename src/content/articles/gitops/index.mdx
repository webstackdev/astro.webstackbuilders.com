---
title: "Gitops Description"
description: "Gitops Description"
cover: "./cover.png"
coverAlt: "TODO"
author: "kevin-brown"
publishDate: 2024-01-15
tags: ["apis-and-gateways"]
featured: true
---

In 2026, separating application code from infrastructure as code (IaC) is standard operational hygiene for ensuring security, governance, and stable automation.

1. What's in the Infrastructure Config Repo?

This repository (often called a "GitOps repo") contains the declarative state of your environment. It typically includes:

- Infrastructure Definitions: Terraform files, OpenTofu modules, or Crossplane compositions that provision the "hardware" (VPC, databases, clusters).
- Kubernetes Manifests: Helm charts, Kustomize overlays, or raw YAML that define how the app runs (CPU/memory limits, ingress rules, environment variables).
- Environment Overlays: Specific folders for dev, staging, and prod to manage differences between environments without duplicating code.
- Policy-as-Code: Rules (e.g., OPA/Rego) that enforce security standards, such as "no public S3 buckets".

2. Changes to Your Application Repo

Your application repo remains the home for feature development, but its CD portion changes significantly:

- CI Stage (Stays Mostly Same): Continues to handle builds, linting, vulnerability scans, and unit/E2E tests.
- CD Stage (Changes): Instead of "pushing" a deployment to Vercel or a cluster, the final step in the application pipeline is now to update the GitOps repo.
- It builds the container image and pushes it to a registry.
- It then creates a Pull Request or directly commits to the Infrastructure Repo, updating the image tag in the manifests (e.g., changing image: v1.0 to image: v1.1).

3. CI/CD in the Infrastructure Repo

The infrastructure repo has its own dedicated lifecycle to ensure changes don't "break the world":

Infra CI Stage:

- Linting/Validation: Tools like tflint or kube-linter check for syntax errors.
- Security Scans: Scanners like tfsec or Checkov search for misconfigurations.
- Dry Runs: Generates a "Plan" (e.g., terraform plan) to show exactly what will be changed before it happens.

Infra CD Stage:

In a GitOps model, there is no traditional "push" CD. Instead, a GitOps Controller (like Argo CD or Flux) inside the cluster watches this repo. The moment it sees a new commit (like the image tag update from your app repo), it pulls and applies those changes to the cluster automatically.

Why this separation?

Security: Developers can have full access to app code but restricted access to production infrastructure.
Stability: Infrastructure changes (like scaling a DB) can be tested and audited separately from code changes (like fixing a typo).

## Typical GitOps Repository Structure (2026)

A production-grade GitOps repository uses a "Base and Overlays" pattern (often with Kustomize or Helm)

```bash
gitops-repo/
├── infrastructure/               # Shared infra-level resources
│   ├── base/                     # Core definitions
│   │   ├── networking.yaml       # VPC/Subnet definitions (Crossplane/Terraform)
│   │   └── database.yaml         # RDS/DB instances
│   └── overlays/                 # Environment-specific tweaks
│       ├── dev/
│       └── prod/                 # e.g., larger DB instance for prod
├── apps/                         # Application manifests
│   ├── my-web-app/
│   │   ├── base/
│   │   │   ├── deployment.yaml   # Basic pod/container spec
│   │   │   └── service.yaml
│   │   └── overlays/
│   │       ├── staging/          # deployment-patch.yaml (low replicas)
│   │       └── production/       # deployment-patch.yaml (high replicas)
├── policies/                     # Policy as Code (PaC)
│   ├── security-rules.rego       # OPA policies (e.g., "no root containers")
│   └── resource-limits.yaml      # Kyverno or Gatekeeper rules
└── clusters/                     # Root entry points for GitOps controller
    ├── staging-cluster/
    │   └── kustomization.yaml    # Points to apps/my-web-app/overlays/staging
    └── prod-cluster/
        └── kustomization.yaml    # Points to apps/my-web-app/overlays/production

```

Immutable Tags: In 2026, standard practice is to use unique, immutable tags—typically the Git Commit SHA (e.g., my-app:a3f5b2c).

OPA, Kyverno, and Gatekeeper

- OPA (Open Policy Agent): An open-source, general-purpose policy engine that unifies policy enforcement across the stack. It uses a declarative language called Rego to define complex rules.
- Gatekeeper: A specialized project that integrates OPA into Kubernetes. It acts as a validating admission controller, intercepting requests to the Kubernetes API and checking them against OPA policies before resources are created or modified.
- Kyverno: A Kubernetes-native policy engine. Unlike Gatekeeper, it does not require learning Rego; policies are written in standard YAML. It can validate, mutate (modify), and generate resources, as well as verify container image signatures.

Here are some widely used Kubernetes control plane extensions:

Infrastructure & Operations Management

- cert-manager: Automates the management and issuance of TLS certificates from various sources like Let's Encrypt, acting as a control plane extension for certificate operations.
- Prometheus Operator: Simplifies the deployment, configuration, and management of the Prometheus monitoring system and related components like Grafana by managing ServiceMonitor and PodMonitor CRDs.
- Cluster Autoscaler: A tool that automatically adjusts the number of worker nodes in your cluster based on the resource requirements of pending pods.
- Velero: Uses CRDs to handle backup and restore procedures for your Kubernetes cluster resources and persistent volumes.

Networking and Security

- Istio / Linkerd (Service Mesh): These add an entire control plane on top of Kubernetes to manage communication between microservices, including traffic management, security (zero-trust), and observability (tracing).
- Nginx/Traefik (Ingress Controllers): While technically a type of controller, they manage external access to services using the Ingress API, a key networking extension point.
- Cilium: A networking, observability, and security solution that uses eBPF technology to enforce network policies and provide advanced insights into cluster traffic.
- Kyverno: A policy management engine that allows you to manage security and compliance policies as Kubernetes resources, validating, mutating, and generating configurations across your cluster.

Cloud Integrations

- Cloud Provider Controllers (AWS Controllers for Kubernetes (ACK), Google Config Connector, Azure Service Operator): Similar in concept to Crossplane, these are specific to each major cloud and allow you to manage cloud-specific infrastructure (e.g., AWS S3, GCP SQL instances) using Kubernetes CRDs.
- Karpenter: An open-source, high-performance autoscaler for Kubernetes that automatically provisions the right compute resources in response to workload needs, especially on AWS.
- Crossplane: an open-source add-on that extends the Kubernetes control plane to manage non-Kubernetes resources, such as cloud infrastructure. It uses Custom Resource Definitions (CRDs) to represent external resources (like AWS S3 buckets or GCP SQL instances) as native Kubernetes objects.

## Using Crossplane instead of Terraform as an article idea

- Those responding here who have the mental agility to understand K8s as a control plane for automation and see how extensible it could be are the exception in the industry not the rule.

- Enterprise wide, though, K8s is not very standardized on. But even in a lot of teams that have it, they just see it as a cloud…those that are creative enough to understand what you could do with controllers and operators beyond just managing workloads are much further and fewer in between. That's the big gap to overcome for Crossplane, IMHO.

In the context of that Reddit thread, users are describing a shift from **Infrastructure as Code (IaC)** as a task to **Kubernetes (K8s) as a Control Plane** as a permanent state. 

While Terraform and Crossplane both use code to define infrastructure, they operate with fundamentally different philosophies: 

1. "Push" vs. "Reconciliation"

- **Terraform (Push):** You run a command (`terraform apply`), Terraform creates the resources, and then it stops. If someone manually deletes an S3 bucket 10 minutes later, Terraform won't know or fix it until you manually run the command again.
- **Crossplane (Reconciliation):** It uses the Kubernetes **Control Loop**. It is "always on." If a resource is deleted or changed manually, the Crossplane controller detects the "drift" and immediately recreates or fixes it to match your code without human intervention. 
- Infrastructure as Data (The Kubernetes API)

The "mental agility" comment refers to seeing K8s not just as a place to run Docker containers, but as a **universal API**. 

- **Custom Resources:** Crossplane extends the K8s API using Custom Resource Definitions (CRDs). An AWS database or an Azure storage bucket becomes a native K8s object, just like a Pod.
- **Standardized Tooling:** You can manage your cloud databases using `kubectl`, apply RBAC policies to them, and manage them with GitOps tools like ArgoCD or Flux, just as you do for your apps. 
- Separation of Concerns (Compositions)

This is the "extensible" part mentioned in the thread. Crossplane allows platform teams to create **Compositions**—high-level abstractions of complex infrastructure. 

- **Platform Team:** Defines a "Standard-DB" composition that includes a database, specific firewall rules, and backup policies.
- **Developers:** Simply request a `Standard-DB` via a small YAML file without needing to know the low-level cloud provider details or having direct access to the cloud console. 

Key Differences at a Glance

| Feature       | Terraform                  | Crossplane                          |
| :------------ | :------------------------- | :---------------------------------- |
| **Execution** | Point-in-time "Apply"      | Continuous reconciliation           |
| **State**     | Stored in an external file | Stored inside K8s as the live state |
| **Interface** | CLI / HCL                  | Kubernetes API / YAML               |
| **Drift**     | Found during next `plan`   | Automatically corrected             |

**Why the "gap"?**
The commenters mean that most people see K8s only as a "hosting platform" (a cloud). They find it difficult to transition to seeing K8s as the **orchestrator for everything**—where the cluster itself acts as a robot that constantly manages all your AWS/GCP/Azure resources. 

"Until composition functions land in Crossplane it’ll still have some major operational holes, like being able to look up which VPC the EKS cluster is in to place the RDS instance in it etc."
