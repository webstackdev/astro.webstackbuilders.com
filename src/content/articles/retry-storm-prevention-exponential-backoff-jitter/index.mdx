---
title: "Retry Storms: Detection, Prevention, Recovery"
description: "How retry storms start, the signals that reveal them, and the patterns that prevent cascade failures."
cover: "./cover.png"
coverAlt: "TODO"
author: "kevin-brown"
publishDate: 2024-01-15
tags: ["reliability-and-testing"]
featured: true
---

*[RPS]: Requests Per Second
*[QPS]: Queries Per Second
*[SLA]: Service Level Agreement
*[SLO]: Service Level Objective
*[P99]: 99th Percentile Latency
*[TCP]: Transmission Control Protocol
*[RTT]: Round-Trip Time
*[MTTR]: Mean Time To Recovery
*[MTBF]: Mean Time Between Failures

Retry amplification math, exponential backoff with jitter, circuit breakers, and the monitoring that catches storms early.

Retries without backoff turn partial failures into total outages.

## The Anatomy of a Retry Storm

How a single slow database query becomes a cascading failure that takes down your entire service mesh.

### Retry Amplification Mathematics

Understanding the exponential math that turns 3 retries per request into a 27x traffic spike.

The amplification factor across N service layers with R retries each: Total Load = Base Load × R^N.

```
Mermaid diagram: Flow diagram showing retry amplification across 3 service layers.
Layer 1 (API Gateway) sends request → Layer 2 (Service A) sends request → Layer 3 (Database).
Database fails → Layer 3 retries 3× → Layer 2 retries 3× (each triggering 3 more DB calls) → Layer 1 retries 3× (each triggering full cascade).
Show the math: 1 → 3 → 9 → 27 requests reaching the database.
```

:::info[The Multiplication Effect]
Every retry at layer N multiplies all retries at layers below it. Three layers with 3 retries each doesn't create 9 retries—it creates 27.
:::

| Retry Configuration | 2-Layer Stack | 3-Layer Stack | 4-Layer Stack |
|---------------------|---------------|---------------|---------------|
| 2 retries per layer | 4× base load | 8× base load | 16× base load |
| 3 retries per layer | 9× base load | 27× base load | 81× base load |
| 5 retries per layer | 25× base load | 125× base load | 625× base load |

### The Timeline of a Typical Storm

Tracing the progression from initial slowdown through complete system failure, with timestamps and metrics.

```
Mermaid diagram: Gantt-style timeline showing retry storm progression.
T+0s: Database connection pool saturates (trigger event).
T+0-5s: Query latency increases from 50ms to 500ms.
T+5-15s: Service A timeouts begin, triggering retries.
T+15-30s: Retry load exceeds normal traffic 10×.
T+30-60s: Service A instance memory exhaustion begins.
T+60-120s: Cascading failures propagate to upstream services.
T+120s+: Complete outage across service mesh.
```

The window between "database is slow" and "everything is down" is measured in seconds, not minutes.

### Why Naive Retries Make Everything Worse

The intuition that "if at first you don't succeed, try again immediately" works for humans but creates catastrophic feedback loops in distributed systems.

Immediate retries arrive at the exact moment the system is least capable of handling additional load.

```typescript
// The dangerous pattern: immediate retry without backoff
async function naiveRetry<T>(
  operation: () => Promise<T>,
  maxRetries: number = 3
): Promise<T> {
  let lastError: Error

  for (let attempt = 0; attempt <= maxRetries; attempt++) {
    try {
      return await operation()
    } catch (error) {
      lastError = error as Error
      // No delay, no backoff, no jitter
      // This GUARANTEES retry storms under load
      console.log(`Attempt ${attempt + 1} failed, retrying immediately...`)
    }
  }

  throw lastError!
}
```

:::danger[This Pattern Causes Outages]
The code above looks reasonable but will amplify any partial failure into a complete outage. Every microservice incident post-mortem I've written traced back to immediate retries somewhere in the call chain.
:::

## Exponential Backoff Fundamentals

The mathematics and implementation of backoff strategies that give overloaded systems time to recover.

### The Math Behind Exponential Growth

Why exponential backoff works: it creates gaps between retry attempts that grow faster than the system's recovery time.

Delay formula: delay = min(base × 2^attempt, maxDelay).

| Attempt | Base 100ms | Base 500ms | Base 1000ms |
|---------|------------|------------|-------------|
| 1 | 100ms | 500ms | 1s |
| 2 | 200ms | 1s | 2s |
| 3 | 400ms | 2s | 4s |
| 4 | 800ms | 4s | 8s |
| 5 | 1.6s | 8s | 16s |
| 6 | 3.2s | 16s | 32s |

```typescript
// Core exponential backoff calculation
function calculateExponentialDelay(
  attempt: number,
  baseDelayMs: number,
  maxDelayMs: number
): number {
  const exponentialDelay = baseDelayMs * Math.pow(2, attempt)
  return Math.min(exponentialDelay, maxDelayMs)
}

// Example usage
const delays = [0, 1, 2, 3, 4, 5].map(attempt =>
  calculateExponentialDelay(attempt, 100, 30000)
)
// Result: [100, 200, 400, 800, 1600, 3200]
```

### Choosing Base Delay and Maximum Delay

Guidelines for selecting backoff parameters based on your system's characteristics and SLO requirements.

Base delay should exceed your P99 latency during normal operation. Maximum delay should be less than your client's patience threshold.

:::warning[Don't Copy-Paste Defaults]
The "standard" 100ms base / 30s max parameters work for some systems and cause problems for others. Your base delay should relate to your actual service latency, not arbitrary round numbers.
:::

```typescript
interface BackoffConfig {
  // Base delay: typically 1.5-2× your P99 latency
  baseDelayMs: number
  // Max delay: user patience threshold minus expected recovery time
  maxDelayMs: number
  // Max attempts: based on total acceptable wait time
  maxAttempts: number
}

function calculateBackoffConfig(
  p99LatencyMs: number,
  userPatienceMs: number,
  expectedRecoveryMs: number
): BackoffConfig {
  const baseDelayMs = Math.round(p99LatencyMs * 1.5)
  const maxDelayMs = Math.round((userPatienceMs - expectedRecoveryMs) / 2)

  // Calculate max attempts to stay within patience threshold
  let totalDelay = 0
  let maxAttempts = 0
  while (totalDelay < userPatienceMs && maxAttempts < 10) {
    totalDelay += Math.min(baseDelayMs * Math.pow(2, maxAttempts), maxDelayMs)
    maxAttempts++
  }

  return { baseDelayMs, maxDelayMs, maxAttempts }
}

// For a service with 50ms P99 and 30s user patience
const config = calculateBackoffConfig(50, 30000, 5000)
// Result: { baseDelayMs: 75, maxDelayMs: 12500, maxAttempts: 8 }
```

### Decorrelated Backoff for Long-Running Operations

When exponential growth is too slow: decorrelated backoff provides faster initial retries with bounded maximum delays.

AWS's decorrelated backoff: delay = min(maxDelay, random_between(base, previousDelay × 3)).

```typescript
// AWS-style decorrelated backoff
function calculateDecorrelatedDelay(
  previousDelayMs: number,
  baseDelayMs: number,
  maxDelayMs: number
): number {
  const minDelay = baseDelayMs
  const maxForThisAttempt = previousDelayMs * 3
  const randomDelay = minDelay + Math.random() * (maxForThisAttempt - minDelay)
  return Math.min(randomDelay, maxDelayMs)
}

class DecorrelatedBackoff {
  private previousDelay: number

  constructor(
    private baseDelayMs: number,
    private maxDelayMs: number
  ) {
    this.previousDelay = baseDelayMs
  }

  nextDelay(): number {
    this.previousDelay = calculateDecorrelatedDelay(
      this.previousDelay,
      this.baseDelayMs,
      this.maxDelayMs
    )
    return this.previousDelay
  }

  reset(): void {
    this.previousDelay = this.baseDelayMs
  }
}
```

## Jitter: Breaking the Thundering Herd

Why adding randomness to retry timing is essential for preventing synchronized retry waves.

### The Thundering Herd Problem

When a thousand clients all retry at exactly the same moment, you've replaced one traffic spike with another.

```
Mermaid diagram: Two scenarios side by side.
LEFT (No Jitter): Timeline showing 100 clients all retrying at exactly T+1s, T+2s, T+4s.
Traffic graph shows sharp spikes at each retry interval.
RIGHT (With Jitter): Same 100 clients with retries spread across time windows.
Traffic graph shows smoothed demand curve without spikes.
```

Without jitter, exponential backoff just schedules the thundering herd for later.

### Full Jitter vs. Equal Jitter vs. Decorrelated Jitter

Comparing the three main jitter strategies and when each is appropriate.

| Strategy | Formula | Best For |
|----------|---------|----------|
| Full Jitter | random(0, exponential_delay) | Most situations, maximum spread |
| Equal Jitter | exponential_delay/2 + random(0, exponential_delay/2) | When minimum delay matters |
| Decorrelated | random(base, previous × 3) | Long-running operations |

```typescript
type JitterStrategy = 'full' | 'equal' | 'decorrelated'

function applyJitter(
  baseDelay: number,
  exponentialDelay: number,
  previousDelay: number,
  strategy: JitterStrategy
): number {
  switch (strategy) {
    case 'full':
      // Delay between 0 and exponential_delay
      return Math.random() * exponentialDelay

    case 'equal':
      // Delay between exponential_delay/2 and exponential_delay
      const halfDelay = exponentialDelay / 2
      return halfDelay + Math.random() * halfDelay

    case 'decorrelated':
      // Delay between base and previous × 3
      const min = baseDelay
      const max = previousDelay * 3
      return min + Math.random() * (max - min)
  }
}
```

:::info[Full Jitter Usually Wins]
AWS's analysis showed full jitter completes work fastest in most scenarios. Equal jitter is only better when you need a guaranteed minimum delay between attempts.
:::

### Implementing Production-Ready Jittered Backoff

A complete TypeScript implementation with all the edge cases handled.

```typescript
interface RetryConfig {
  baseDelayMs: number
  maxDelayMs: number
  maxAttempts: number
  jitterStrategy: 'full' | 'equal' | 'decorrelated'
  // Which errors should trigger retries
  retryableErrors?: (error: Error) => boolean
  // Called before each retry for logging/metrics
  onRetry?: (attempt: number, delay: number, error: Error) => void
}

const defaultConfig: RetryConfig = {
  baseDelayMs: 100,
  maxDelayMs: 30000,
  maxAttempts: 5,
  jitterStrategy: 'full',
  retryableErrors: (error) => {
    // Default: retry on network errors and 5xx responses
    if (error.name === 'NetworkError') return true
    if ('status' in error && (error as any).status >= 500) return true
    return false
  }
}

async function retryWithBackoff<T>(
  operation: () => Promise<T>,
  config: Partial<RetryConfig> = {}
): Promise<T> {
  const cfg = { ...defaultConfig, ...config }
  let lastError: Error
  let previousDelay = cfg.baseDelayMs

  for (let attempt = 0; attempt < cfg.maxAttempts; attempt++) {
    try {
      return await operation()
    } catch (error) {
      lastError = error as Error

      // Don't retry non-retryable errors
      if (cfg.retryableErrors && !cfg.retryableErrors(lastError)) {
        throw lastError
      }

      // Don't wait after the last attempt
      if (attempt === cfg.maxAttempts - 1) {
        break
      }

      // Calculate delay with jitter
      const exponentialDelay = Math.min(
        cfg.baseDelayMs * Math.pow(2, attempt),
        cfg.maxDelayMs
      )

      const delay = applyJitter(
        cfg.baseDelayMs,
        exponentialDelay,
        previousDelay,
        cfg.jitterStrategy
      )

      previousDelay = delay
      cfg.onRetry?.(attempt + 1, delay, lastError)

      await sleep(delay)
    }
  }

  throw lastError!
}

function sleep(ms: number): Promise<void> {
  return new Promise(resolve => setTimeout(resolve, ms))
}
```

## Circuit Breakers: Knowing When to Stop

Circuit breakers prevent retry storms by detecting when retries are futile and stopping them at the source.

### Circuit Breaker State Machine

The three states of a circuit breaker and the conditions that trigger transitions between them.

```
Mermaid diagram: State machine with three states.
CLOSED (normal operation) → counts failures
  → Failure threshold exceeded → OPEN
OPEN (failing fast, no requests sent)
  → Timeout expires → HALF-OPEN
HALF-OPEN (testing recovery)
  → Probe request succeeds → CLOSED
  → Probe request fails → OPEN
Show failure count, success count, and timer on relevant transitions.
```

A circuit breaker is a state machine that tracks recent failure rates and blocks requests when failure becomes likely.

### Implementing a Circuit Breaker

TypeScript implementation with configurable thresholds and timeout behavior.

```typescript
type CircuitState = 'closed' | 'open' | 'half-open'

interface CircuitBreakerConfig {
  failureThreshold: number      // Failures before opening
  successThreshold: number      // Successes in half-open before closing
  timeout: number               // Ms before transitioning open → half-open
  volumeThreshold: number       // Min requests before evaluating failure rate
}

class CircuitBreaker {
  private state: CircuitState = 'closed'
  private failures = 0
  private successes = 0
  private totalRequests = 0
  private lastFailureTime = 0
  private nextAttemptTime = 0

  constructor(private config: CircuitBreakerConfig) {}

  async execute<T>(operation: () => Promise<T>): Promise<T> {
    if (!this.canExecute()) {
      throw new CircuitOpenError(
        `Circuit open, retry after ${this.nextAttemptTime - Date.now()}ms`
      )
    }

    try {
      const result = await operation()
      this.recordSuccess()
      return result
    } catch (error) {
      this.recordFailure()
      throw error
    }
  }

  private canExecute(): boolean {
    if (this.state === 'closed') return true

    if (this.state === 'open') {
      if (Date.now() >= this.nextAttemptTime) {
        this.state = 'half-open'
        this.successes = 0
        return true
      }
      return false
    }

    // half-open: allow limited requests to test recovery
    return true
  }

  private recordSuccess(): void {
    this.totalRequests++

    if (this.state === 'half-open') {
      this.successes++
      if (this.successes >= this.config.successThreshold) {
        this.reset()
      }
    }
  }

  private recordFailure(): void {
    this.totalRequests++
    this.failures++
    this.lastFailureTime = Date.now()

    if (this.state === 'half-open') {
      this.tripBreaker()
      return
    }

    // Only evaluate in closed state with sufficient volume
    if (this.state === 'closed' &&
        this.totalRequests >= this.config.volumeThreshold) {
      if (this.failures >= this.config.failureThreshold) {
        this.tripBreaker()
      }
    }
  }

  private tripBreaker(): void {
    this.state = 'open'
    this.nextAttemptTime = Date.now() + this.config.timeout
  }

  private reset(): void {
    this.state = 'closed'
    this.failures = 0
    this.successes = 0
    this.totalRequests = 0
  }

  getState(): { state: CircuitState; failures: number; nextAttempt?: number } {
    return {
      state: this.state,
      failures: this.failures,
      nextAttempt: this.state === 'open' ? this.nextAttemptTime : undefined
    }
  }
}

class CircuitOpenError extends Error {
  constructor(message: string) {
    super(message)
    this.name = 'CircuitOpenError'
  }
}
```

### Combining Circuit Breakers with Retries

The correct order of operations: circuit breaker wraps retry logic, not the other way around.

```typescript
// CORRECT: Circuit breaker wraps the retrying operation
async function resilientCall<T>(
  operation: () => Promise<T>,
  circuitBreaker: CircuitBreaker,
  retryConfig: RetryConfig
): Promise<T> {
  return circuitBreaker.execute(() =>
    retryWithBackoff(operation, retryConfig)
  )
}

// WRONG: Retrying circuit breaker calls
// This defeats the purpose—you'd retry even when the circuit is open
async function wrongApproach<T>(
  operation: () => Promise<T>,
  circuitBreaker: CircuitBreaker
): Promise<T> {
  return retryWithBackoff(
    () => circuitBreaker.execute(operation), // DON'T DO THIS
    { maxAttempts: 3 }
  )
}
```

:::warning[Order Matters]
If you retry circuit breaker calls, you waste retry attempts on immediate CircuitOpenError rejections. The circuit breaker must wrap the entire retry sequence.
:::

### Per-Endpoint Circuit Breakers

One circuit breaker per dependency endpoint prevents a single failing endpoint from blocking healthy ones.

```typescript
class CircuitBreakerRegistry {
  private breakers = new Map<string, CircuitBreaker>()
  private defaultConfig: CircuitBreakerConfig

  constructor(defaultConfig: CircuitBreakerConfig) {
    this.defaultConfig = defaultConfig
  }

  getBreaker(
    endpoint: string,
    config?: Partial<CircuitBreakerConfig>
  ): CircuitBreaker {
    if (!this.breakers.has(endpoint)) {
      this.breakers.set(
        endpoint,
        new CircuitBreaker({ ...this.defaultConfig, ...config })
      )
    }
    return this.breakers.get(endpoint)!
  }

  getAllStates(): Map<string, ReturnType<CircuitBreaker['getState']>> {
    const states = new Map()
    for (const [endpoint, breaker] of this.breakers) {
      states.set(endpoint, breaker.getState())
    }
    return states
  }
}

// Usage
const registry = new CircuitBreakerRegistry({
  failureThreshold: 5,
  successThreshold: 2,
  timeout: 30000,
  volumeThreshold: 10
})

// Each endpoint gets its own circuit breaker
const userServiceBreaker = registry.getBreaker('user-service/api/users')
const paymentBreaker = registry.getBreaker('payment-service/api/charge', {
  failureThreshold: 3,  // More sensitive for payment operations
  timeout: 60000        // Longer recovery for payment provider issues
})
```

## Retry Budget: System-Wide Coordination

When individual retry logic is correct but collective retries still overwhelm the system.

### Token Bucket Retry Budgets

Limiting total retry attempts across all clients using a shared token bucket.

```
Mermaid diagram: Token bucket for retry budget.
Bucket with capacity of 100 tokens, refill rate of 10 tokens/second.
Multiple clients (Client A, B, C) requesting tokens before retrying.
Show scenarios:
1. Client A requests token, bucket has 50 → granted, bucket now 49
2. Client B requests token, bucket has 1 → granted, bucket now 0
3. Client C requests token, bucket empty → retry blocked
Refill arrow showing continuous token addition.
```

A retry budget ensures the system never exceeds a defined retry rate, regardless of how many clients are retrying.

```typescript
class RetryBudget {
  private tokens: number
  private lastRefillTime: number

  constructor(
    private capacity: number,       // Maximum tokens in bucket
    private refillRate: number,     // Tokens added per second
    private minRetryRatio: number   // Min % of requests that can retry
  ) {
    this.tokens = capacity
    this.lastRefillTime = Date.now()
  }

  tryAcquire(): boolean {
    this.refill()

    if (this.tokens >= 1) {
      this.tokens -= 1
      return true
    }

    return false
  }

  private refill(): void {
    const now = Date.now()
    const elapsed = (now - this.lastRefillTime) / 1000
    const tokensToAdd = elapsed * this.refillRate

    this.tokens = Math.min(this.capacity, this.tokens + tokensToAdd)
    this.lastRefillTime = now
  }

  // For monitoring
  getAvailableTokens(): number {
    this.refill()
    return this.tokens
  }

  getUtilization(): number {
    return 1 - (this.getAvailableTokens() / this.capacity)
  }
}

// Retry only if budget allows
async function budgetedRetry<T>(
  operation: () => Promise<T>,
  budget: RetryBudget,
  config: RetryConfig
): Promise<T> {
  let lastError: Error

  for (let attempt = 0; attempt < config.maxAttempts; attempt++) {
    try {
      return await operation()
    } catch (error) {
      lastError = error as Error

      // First attempt doesn't consume budget (not a retry)
      if (attempt > 0 && !budget.tryAcquire()) {
        // Budget exhausted, fail immediately
        throw new RetryBudgetExhaustedError(
          `Retry budget exhausted after ${attempt} attempts`,
          lastError
        )
      }

      if (attempt < config.maxAttempts - 1) {
        const delay = calculateBackoffWithJitter(attempt, config)
        await sleep(delay)
      }
    }
  }

  throw lastError!
}
```

### Distributed Retry Coordination with Redis

Sharing retry budget across multiple service instances.

```typescript
import { Redis } from 'ioredis'

class DistributedRetryBudget {
  constructor(
    private redis: Redis,
    private key: string,
    private capacity: number,
    private refillRate: number,
    private windowMs: number = 1000
  ) {}

  async tryAcquire(): Promise<boolean> {
    // Lua script for atomic token bucket
    const script = `
      local key = KEYS[1]
      local capacity = tonumber(ARGV[1])
      local refillRate = tonumber(ARGV[2])
      local now = tonumber(ARGV[3])
      local windowMs = tonumber(ARGV[4])

      local bucket = redis.call('HMGET', key, 'tokens', 'lastRefill')
      local tokens = tonumber(bucket[1]) or capacity
      local lastRefill = tonumber(bucket[2]) or now

      -- Refill tokens based on elapsed time
      local elapsed = (now - lastRefill) / 1000
      local newTokens = math.min(capacity, tokens + elapsed * refillRate)

      if newTokens >= 1 then
        redis.call('HMSET', key, 'tokens', newTokens - 1, 'lastRefill', now)
        redis.call('PEXPIRE', key, windowMs * 10)
        return 1
      else
        redis.call('HMSET', key, 'tokens', newTokens, 'lastRefill', now)
        redis.call('PEXPIRE', key, windowMs * 10)
        return 0
      end
    `

    const result = await this.redis.eval(
      script,
      1,
      this.key,
      this.capacity,
      this.refillRate,
      Date.now(),
      this.windowMs
    )

    return result === 1
  }
}
```

### Adaptive Retry Budgets

Dynamically adjusting retry budgets based on downstream health signals.

```typescript
class AdaptiveRetryBudget {
  private budget: RetryBudget
  private errorRate = 0
  private requestCount = 0
  private errorCount = 0
  private readonly windowSize = 100

  constructor(
    private baseCapacity: number,
    private baseRefillRate: number
  ) {
    this.budget = new RetryBudget(baseCapacity, baseRefillRate, 0.1)
  }

  recordSuccess(): void {
    this.requestCount++
    this.updateErrorRate()
    this.adjustBudget()
  }

  recordFailure(): void {
    this.requestCount++
    this.errorCount++
    this.updateErrorRate()
    this.adjustBudget()
  }

  private updateErrorRate(): void {
    if (this.requestCount >= this.windowSize) {
      this.errorRate = this.errorCount / this.requestCount
      // Sliding window: decay old data
      this.requestCount = Math.floor(this.requestCount * 0.5)
      this.errorCount = Math.floor(this.errorCount * 0.5)
    }
  }

  private adjustBudget(): void {
    // Reduce budget as error rate increases
    // At 50% error rate, budget is 0.5× base
    // At 0% error rate, budget is 1× base
    const multiplier = 1 - this.errorRate

    const adjustedCapacity = Math.max(1,
      Math.floor(this.baseCapacity * multiplier)
    )
    const adjustedRefillRate = Math.max(0.1,
      this.baseRefillRate * multiplier
    )

    this.budget = new RetryBudget(adjustedCapacity, adjustedRefillRate, 0.1)
  }

  tryAcquire(): boolean {
    return this.budget.tryAcquire()
  }

  getStats(): { errorRate: number; budgetUtilization: number } {
    return {
      errorRate: this.errorRate,
      budgetUtilization: this.budget.getUtilization()
    }
  }
}
```

## Detecting Retry Storms in Progress

The metrics and alerts that catch storms before they become outages.

### Key Metrics for Storm Detection

The signals that distinguish normal retry behavior from a developing storm.

| Metric | Normal Range | Warning | Critical |
|--------|--------------|---------|----------|
| Retry rate (retries/second) | < 1% of RPS | 5% of RPS | > 10% of RPS |
| Retry amplification factor | 1.0 - 1.1× | 1.5× | > 2× |
| Circuit breaker open rate | 0% | 5% of breakers | > 20% of breakers |
| Retry budget utilization | < 50% | 80% | > 95% |
| P99 latency increase | < 1.5× baseline | 3× baseline | > 10× baseline |

```typescript
// Prometheus metrics for retry monitoring
interface RetryMetrics {
  // Counter: total retry attempts by endpoint and status
  retryAttempts: Counter<{ endpoint: string; status: 'success' | 'failure' }>
  // Histogram: delay between retries
  retryDelay: Histogram<{ endpoint: string }>
  // Gauge: current retry budget utilization
  retryBudgetUtilization: Gauge<{ service: string }>
  // Gauge: circuit breaker state
  circuitBreakerState: Gauge<{ endpoint: string; state: string }>
}

// Recording rules for derived metrics
const prometheusRules = `
groups:
  - name: retry_storm_detection
    rules:
      # Retry rate as percentage of total requests
      - record: service:retry_rate:ratio
        expr: |
          sum(rate(retry_attempts_total[5m])) by (service)
          /
          sum(rate(http_requests_total[5m])) by (service)

      # Retry amplification factor
      - record: service:retry_amplification:ratio
        expr: |
          (sum(rate(http_requests_total[5m])) by (service)
           + sum(rate(retry_attempts_total[5m])) by (service))
          /
          sum(rate(http_requests_total[5m])) by (service)

      # Circuit breaker open percentage
      - record: service:circuit_breakers_open:ratio
        expr: |
          sum(circuit_breaker_state{state="open"}) by (service)
          /
          count(circuit_breaker_state) by (service)
`
```

### Real-Time Storm Detection Alerts

PromQL queries for detecting and alerting on retry storm patterns.

```yaml
# Prometheus alerting rules
groups:
  - name: retry_storm_alerts
    rules:
      - alert: RetryStormDeveloping
        expr: service:retry_rate:ratio > 0.05
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Elevated retry rate detected"
          description: "Service {{ $labels.service }} retry rate is {{ $value | humanizePercentage }}"

      - alert: RetryStormActive
        expr: service:retry_amplification:ratio > 2
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Retry storm in progress"
          description: "Service {{ $labels.service }} traffic amplified {{ $value }}×"
          runbook_url: "https://runbooks.example.com/retry-storm"

      - alert: RetryBudgetExhausted
        expr: retry_budget_utilization > 0.95
        for: 30s
        labels:
          severity: critical
        annotations:
          summary: "Retry budget nearly exhausted"
          description: "Retry budget for {{ $labels.service }} at {{ $value | humanizePercentage }}"

      - alert: CircuitBreakersTripping
        expr: service:circuit_breakers_open:ratio > 0.2
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "Multiple circuit breakers open"
          description: "{{ $value | humanizePercentage }} of circuit breakers open for {{ $labels.service }}"
```

### Distributed Tracing for Retry Analysis

Using tracing to understand retry patterns across service boundaries.

```typescript
import { trace, SpanKind, SpanStatusCode } from '@opentelemetry/api'

const tracer = trace.getTracer('retry-instrumentation')

async function tracedRetry<T>(
  operation: () => Promise<T>,
  config: RetryConfig,
  operationName: string
): Promise<T> {
  return tracer.startActiveSpan(
    `retry:${operationName}`,
    { kind: SpanKind.INTERNAL },
    async (parentSpan) => {
      let lastError: Error

      for (let attempt = 0; attempt < config.maxAttempts; attempt++) {
        const attemptSpan = tracer.startSpan(`attempt-${attempt + 1}`, {
          kind: SpanKind.INTERNAL,
          attributes: {
            'retry.attempt': attempt + 1,
            'retry.max_attempts': config.maxAttempts
          }
        })

        try {
          const result = await operation()
          attemptSpan.setStatus({ code: SpanStatusCode.OK })
          attemptSpan.end()

          parentSpan.setAttributes({
            'retry.total_attempts': attempt + 1,
            'retry.final_status': 'success'
          })
          parentSpan.end()

          return result
        } catch (error) {
          lastError = error as Error
          attemptSpan.recordException(lastError)
          attemptSpan.setStatus({
            code: SpanStatusCode.ERROR,
            message: lastError.message
          })
          attemptSpan.end()

          if (attempt < config.maxAttempts - 1) {
            const delay = calculateBackoffWithJitter(attempt, config)
            parentSpan.addEvent('retry_scheduled', {
              'retry.delay_ms': delay,
              'retry.reason': lastError.message
            })
            await sleep(delay)
          }
        }
      }

      parentSpan.setAttributes({
        'retry.total_attempts': config.maxAttempts,
        'retry.final_status': 'exhausted'
      })
      parentSpan.end()

      throw lastError!
    }
  )
}
```

## Recovery Strategies During Active Storms

What to do when prevention fails and you're in the middle of a retry storm.

### Immediate Response: Shed Load

The first response to an active storm is reducing load at the edges.

```typescript
// Emergency load shedding middleware
class LoadShedder {
  private currentLoad = 0
  private readonly maxLoad: number
  private readonly shedProbability: () => number

  constructor(maxLoad: number) {
    this.maxLoad = maxLoad
    // Probability increases exponentially as load approaches max
    this.shedProbability = () => {
      const loadRatio = this.currentLoad / this.maxLoad
      if (loadRatio < 0.8) return 0
      return Math.pow((loadRatio - 0.8) / 0.2, 2)
    }
  }

  shouldShedRequest(): boolean {
    return Math.random() < this.shedProbability()
  }

  trackRequest(): () => void {
    this.currentLoad++
    return () => { this.currentLoad-- }
  }
}

// Express middleware for load shedding
function loadSheddingMiddleware(shedder: LoadShedder) {
  return (req: Request, res: Response, next: NextFunction) => {
    if (shedder.shouldShedRequest()) {
      res.status(503).set({
        'Retry-After': '30',
        'X-Load-Shed': 'true'
      }).json({
        error: 'Service temporarily unavailable',
        retryAfter: 30
      })
      return
    }

    const release = shedder.trackRequest()
    res.on('finish', release)
    next()
  }
}
```

:::danger[Shed Early, Shed at the Edge]
Load shedding at the database is too late—the request has already consumed resources at every layer. Shed at your API gateway or load balancer.
:::

### Graceful Degradation Patterns

Returning cached or degraded responses instead of failing completely.

```typescript
interface DegradedResponse<T> {
  data: T
  degraded: boolean
  reason?: string
  staleAge?: number
}

class GracefulDegradation<T> {
  private cache: Map<string, { data: T; timestamp: number }> = new Map()

  constructor(
    private fetchFresh: (key: string) => Promise<T>,
    private staleTtlMs: number = 300000,  // 5 minutes
    private degradedTtlMs: number = 3600000  // 1 hour for degraded mode
  ) {}

  async get(key: string, allowDegraded = true): Promise<DegradedResponse<T>> {
    const cached = this.cache.get(key)
    const now = Date.now()

    try {
      const fresh = await this.fetchFresh(key)
      this.cache.set(key, { data: fresh, timestamp: now })
      return { data: fresh, degraded: false }
    } catch (error) {
      if (!allowDegraded || !cached) {
        throw error
      }

      const age = now - cached.timestamp

      if (age < this.degradedTtlMs) {
        return {
          data: cached.data,
          degraded: true,
          reason: 'upstream_failure',
          staleAge: age
        }
      }

      throw error
    }
  }
}
```

### Coordinated Recovery: Backpressure Signals

Communicating overload status upstream to prevent continued retry pressure.

```
Mermaid diagram: Backpressure signal flow.
Database → signals overload to Service B (via error rate, latency)
Service B → opens circuit breaker, signals via headers to Service A
Service A → reduces retry rate, signals via 503 + Retry-After to API Gateway
API Gateway → applies rate limiting to clients
Show the cascade of backpressure reducing load at each layer.
```

```typescript
// Backpressure headers middleware
interface BackpressureSignal {
  overloaded: boolean
  retryAfter?: number
  capacityRemaining?: number
}

function backpressureMiddleware(
  getSignal: () => BackpressureSignal
) {
  return (req: Request, res: Response, next: NextFunction) => {
    const signal = getSignal()

    // Always include capacity headers for upstream visibility
    res.set({
      'X-Capacity-Remaining': String(signal.capacityRemaining ?? 100),
      'X-Overloaded': String(signal.overloaded)
    })

    if (signal.overloaded && signal.retryAfter) {
      res.set('Retry-After', String(signal.retryAfter))
    }

    next()
  }
}

// Upstream client respects backpressure
async function backpressureAwareRequest(
  url: string,
  retryConfig: RetryConfig
): Promise<Response> {
  const response = await fetch(url)

  // Check for backpressure signals
  const overloaded = response.headers.get('X-Overloaded') === 'true'
  const retryAfter = parseInt(response.headers.get('Retry-After') || '0', 10)

  if (overloaded) {
    // Increase backoff for subsequent requests
    retryConfig.baseDelayMs = Math.max(
      retryConfig.baseDelayMs,
      retryAfter * 1000
    )
  }

  return response
}
```

## Testing Retry Behavior

Validating that your retry logic works correctly under failure conditions.

### Chaos Engineering for Retries

Injecting failures to verify retry behavior in realistic scenarios.

```typescript
// Failure injection for testing
class ChaosProxy<T> {
  constructor(
    private target: T,
    private config: ChaosConfig
  ) {}

  wrap(): T {
    return new Proxy(this.target as object, {
      get: (target, prop) => {
        const original = (target as any)[prop]

        if (typeof original !== 'function') return original

        return async (...args: any[]) => {
          // Random failure injection
          if (Math.random() < this.config.failureRate) {
            const delay = this.config.minLatencyMs +
              Math.random() * (this.config.maxLatencyMs - this.config.minLatencyMs)
            await sleep(delay)
            throw new Error(`Chaos: injected failure for ${String(prop)}`)
          }

          // Latency injection
          if (Math.random() < this.config.latencyRate) {
            const delay = this.config.minLatencyMs +
              Math.random() * (this.config.maxLatencyMs - this.config.minLatencyMs)
            await sleep(delay)
          }

          return original.apply(target, args)
        }
      }
    }) as T
  }
}

interface ChaosConfig {
  failureRate: number      // 0-1, probability of failure
  latencyRate: number      // 0-1, probability of added latency
  minLatencyMs: number
  maxLatencyMs: number
}

// Usage in tests
const chaosConfig: ChaosConfig = {
  failureRate: 0.3,        // 30% of calls fail
  latencyRate: 0.5,        // 50% of calls have added latency
  minLatencyMs: 100,
  maxLatencyMs: 5000
}

const chaosDb = new ChaosProxy(database, chaosConfig).wrap()
```

### Unit Testing Retry Logic

Testing backoff calculations and jitter distributions.

```typescript
import { describe, it, expect, vi } from 'vitest'

describe('Exponential Backoff', () => {
  it('doubles delay on each attempt', () => {
    const delays = [0, 1, 2, 3, 4].map(attempt =>
      calculateExponentialDelay(attempt, 100, 30000)
    )
    expect(delays).toEqual([100, 200, 400, 800, 1600])
  })

  it('respects maximum delay', () => {
    const delay = calculateExponentialDelay(10, 100, 1000)
    expect(delay).toBe(1000)
  })
})

describe('Jitter Distribution', () => {
  it('full jitter stays within bounds', () => {
    const samples = Array.from({ length: 1000 }, () =>
      applyJitter(100, 1000, 500, 'full')
    )

    expect(Math.min(...samples)).toBeGreaterThanOrEqual(0)
    expect(Math.max(...samples)).toBeLessThanOrEqual(1000)
  })

  it('equal jitter has minimum floor', () => {
    const samples = Array.from({ length: 1000 }, () =>
      applyJitter(100, 1000, 500, 'equal')
    )

    // Equal jitter minimum is exponentialDelay / 2
    expect(Math.min(...samples)).toBeGreaterThanOrEqual(500)
  })

  it('jitter provides reasonable distribution', () => {
    const samples = Array.from({ length: 10000 }, () =>
      applyJitter(100, 1000, 500, 'full')
    )

    const mean = samples.reduce((a, b) => a + b, 0) / samples.length
    // Full jitter mean should be approximately exponentialDelay / 2
    expect(mean).toBeGreaterThan(400)
    expect(mean).toBeLessThan(600)
  })
})

describe('Circuit Breaker', () => {
  it('opens after failure threshold', async () => {
    const breaker = new CircuitBreaker({
      failureThreshold: 3,
      successThreshold: 2,
      timeout: 1000,
      volumeThreshold: 1
    })

    const failingOperation = () => Promise.reject(new Error('fail'))

    // Trip the breaker
    for (let i = 0; i < 3; i++) {
      await expect(breaker.execute(failingOperation)).rejects.toThrow()
    }

    // Next call should fail immediately with circuit open
    await expect(breaker.execute(failingOperation))
      .rejects.toThrow(CircuitOpenError)

    expect(breaker.getState().state).toBe('open')
  })

  it('transitions to half-open after timeout', async () => {
    vi.useFakeTimers()

    const breaker = new CircuitBreaker({
      failureThreshold: 1,
      successThreshold: 1,
      timeout: 1000,
      volumeThreshold: 1
    })

    // Trip the breaker
    await expect(
      breaker.execute(() => Promise.reject(new Error('fail')))
    ).rejects.toThrow()

    expect(breaker.getState().state).toBe('open')

    // Advance past timeout
    vi.advanceTimersByTime(1001)

    // Next call should be allowed (half-open)
    await breaker.execute(() => Promise.resolve('success'))
    expect(breaker.getState().state).toBe('closed')

    vi.useRealTimers()
  })
})
```

### Load Testing Retry Behavior

Simulating retry storms to verify system resilience.

```typescript
// k6 load test for retry storm simulation
const k6Script = `
import http from 'k6/http'
import { check, sleep } from 'k6'
import { Rate, Trend } from 'k6/metrics'

const retryRate = new Rate('retry_rate')
const retryCount = new Trend('retry_count')

export const options = {
  scenarios: {
    // Normal load baseline
    normal_load: {
      executor: 'constant-vus',
      vus: 50,
      duration: '5m',
      startTime: '0s'
    },
    // Simulate partial failure causing retries
    retry_storm: {
      executor: 'ramping-vus',
      startVUs: 0,
      stages: [
        { duration: '1m', target: 200 },  // Ramp up
        { duration: '3m', target: 200 },  // Sustained high load
        { duration: '1m', target: 0 }     // Ramp down
      ],
      startTime: '5m'
    }
  },
  thresholds: {
    http_req_duration: ['p(99)<1000'],
    retry_rate: ['rate<0.1'],  // Less than 10% retries
    'http_req_failed': ['rate<0.05']
  }
}

export default function() {
  let attempts = 0
  let success = false
  const maxAttempts = 3
  const baseDelay = 100

  while (attempts < maxAttempts && !success) {
    const response = http.get('http://localhost:3000/api/resource')
    attempts++

    if (response.status === 200) {
      success = true
    } else if (response.status >= 500 && attempts < maxAttempts) {
      // Exponential backoff with jitter
      const delay = baseDelay * Math.pow(2, attempts - 1)
      const jitter = delay * Math.random()
      sleep(jitter / 1000)
    }
  }

  retryRate.add(attempts > 1)
  retryCount.add(attempts)

  check(response, {
    'status is 200': (r) => r.status === 200,
    'no excessive retries': () => attempts <= 2
  })
}
`
```

## Conclusion

Retry storms convert recoverable failures into total outages through exponential amplification. Prevention requires understanding the math (retries multiply across layers), implementing proper backoff with jitter (to prevent thundering herds), and coordinating limits (circuit breakers and retry budgets). Detection requires monitoring retry rates, amplification factors, and circuit breaker states. Recovery requires load shedding, graceful degradation, and backpressure signaling. The difference between a 5-minute degradation and a 2-hour outage often comes down to whether your retry logic creates feedback loops or breaks them.

---

## Cover Image Prompts

### Prompt 1: Storm Breaking Against Architecture
A dramatic photograph of a massive wave crashing against a geometric concrete seawall, with the spray pattern frozen mid-impact showing chaotic water fragments against the structured defense. Deep blue-grey tones with white foam highlights. The concrete shows mathematical precision while the water shows pure chaos being deflected.

### Prompt 2: Dampening Mechanism Technical Diagram
Technical blueprint-style illustration of a shock absorber or dampening system in cross-section, showing springs, hydraulic chambers, and energy dissipation paths. Rendered in white lines on deep navy blue background with orange accent lines showing force flow. Mathematical annotations for dampening coefficients visible in margins.

### Prompt 3: Signal Propagation Decay
Abstract visualization of a signal or ripple pattern that starts intense at center and progressively dampens toward edges. Concentric rings transitioning from bright red/orange core through yellow to cool blue at periphery. Clean geometric rendering with slight motion blur suggesting energy dissipation over time.

### Prompt 4: Circuit Breaker Industrial Detail
Close-up photograph of an industrial high-voltage circuit breaker switch in the open position, showing the gap between contacts. Dramatic side lighting creating strong shadows. Copper contacts with slight oxidation patina against matte grey housing. Shallow depth of field with warning labels slightly blurred.

### Prompt 5: Feedback Loop Interruption
Abstract mathematical visualization showing a spiral pattern that normally converges inward but is interrupted by a barrier element, causing the spiral to deflect outward and dissipate. Gold/amber spiral lines against deep purple background. The interruption point highlighted with a subtle glow effect.
