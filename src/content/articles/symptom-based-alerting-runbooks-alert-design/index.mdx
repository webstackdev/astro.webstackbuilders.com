---
title: "Alert Fatigue: Symptom-Based Alerting That Works"
description: "Designing alerts that wake people up for real problems and include runbooks for resolution."
cover: "./cover.png"
coverAlt: "TODO"
author: "kevin-brown"
publishDate: 2024-01-15
tags: ["observability-and-telemetry"]
featured: true
---

*[MTTA]: Mean Time to Acknowledge
*[MTTR]: Mean Time to Resolve
*[NOC]: Network Operations Center
*[P1]: Priority 1 (Critical)
*[P2]: Priority 2 (High)
*[P3]: Priority 3 (Medium)
*[SLA]: Service Level Agreement
*[SLI]: Service Level Indicator
*[SLO]: Service Level Objective
*[SNR]: Signal to Noise Ratio

Symptom vs cause alerting, signal selection, threshold tuning, and runbook integration that makes alerts actionable.

Alert on what matters to users, not what is easy to measure.

## The Alert Fatigue Problem

When everything alerts, nothing alerts—teams learn to ignore pagers that cry wolf.

### Anatomy of Alert Fatigue

How well-intentioned monitoring becomes noise.

```typescript
// Alert fatigue progression
interface AlertFatigueStage {
  stage: string
  alertsPerDay: number
  actionableRate: number
  teamBehavior: string
  consequence: string
}

const fatigueProgression: AlertFatigueStage[] = [
  {
    stage: 'Initial deployment',
    alertsPerDay: 5,
    actionableRate: 0.9,
    teamBehavior: 'Investigate every alert thoroughly',
    consequence: 'High trust in alerting system'
  },
  {
    stage: 'Alert creep',
    alertsPerDay: 25,
    actionableRate: 0.6,
    teamBehavior: 'Quick triage, some alerts ignored',
    consequence: 'Alert response time increases'
  },
  {
    stage: 'Noise dominance',
    alertsPerDay: 100,
    actionableRate: 0.2,
    teamBehavior: 'Batch acknowledge, selective response',
    consequence: 'Real incidents missed in noise'
  },
  {
    stage: 'Learned helplessness',
    alertsPerDay: 200,
    actionableRate: 0.05,
    teamBehavior: 'Mute channels, ignore pages',
    consequence: 'Alerting system abandoned'
  }
]
```

```
Mermaid diagram: Alert fatigue cycle.
"We had an incident"
       ↓
"Add an alert for that"
       ↓
Alerts increase
       ↓
More false positives
       ↓
Team starts ignoring alerts
       ↓
Real incident missed
       ↓
"We need more alerts"
       ↓
(cycle repeats)

Breaking the cycle requires:
- Alert on symptoms, not causes
- Require runbooks for all alerts
- Regular alert hygiene reviews
```

### Measuring Alert Quality

Metrics that reveal whether your alerts are helping or hurting.

| Metric | Definition | Healthy Target | Warning Sign |
|--------|------------|----------------|--------------|
| Alert volume | Alerts per on-call shift | < 5 pages/day | > 10 pages/day |
| Actionable rate | % alerts requiring human action | > 80% | < 50% |
| False positive rate | % alerts with no real impact | < 10% | > 30% |
| MTTA | Time to acknowledge | < 5 min | > 15 min |
| MTTR | Time from alert to resolution | < 30 min | > 2 hours |
| Duplicate rate | % alerts for same incident | < 10% | > 25% |
| Snooze rate | % alerts snoozed without action | < 5% | > 20% |

```typescript
// Alert quality dashboard query
interface AlertQualityMetrics {
  period: string
  totalAlerts: number
  actionable: number
  falsePositive: number
  duplicates: number
  snoozed: number
  avgAcknowledgeTime: number
  avgResolveTime: number
}

// Prometheus queries for alert metrics
const alertMetricQueries = {
  totalAlerts: `
    count(ALERTS{alertstate="firing"})
  `,

  actionableRate: `
    sum(alert_outcome{outcome="action_taken"})
    / sum(alert_outcome) * 100
  `,

  falsePositiveRate: `
    sum(alert_outcome{outcome="false_positive"})
    / sum(alert_outcome) * 100
  `,

  meanTimeToAcknowledge: `
    avg(alert_acknowledged_timestamp - alert_fired_timestamp)
  `,

  alertsPerOncallShift: `
    sum(increase(alerts_total[8h]))
    / count(distinct(oncall_engineer))
  `
}
```

<Callout type="warning">
If your on-call receives more than 5 pages per day on average, you have an alerting problem. Each additional alert reduces the attention given to all alerts. Fix the noisy alerts before adding new ones.
</Callout>

## Symptom vs Cause Alerting

Alert on what users experience, not what might cause problems.

### The Symptom-Based Philosophy

Why alerting on causes creates noise while symptom-based alerting catches problems.

```typescript
// Cause-based vs symptom-based alerting
interface AlertComparison {
  causeBasedAlert: Alert
  symptomBasedAlert: Alert
  tradeoffs: string[]
}

const alertComparisons: AlertComparison[] = [
  {
    causeBasedAlert: {
      name: 'High CPU usage',
      condition: 'cpu_usage > 90%',
      problem: 'Fires during legitimate load spikes, normal GC, batch jobs'
    },
    symptomBasedAlert: {
      name: 'Elevated request latency',
      condition: 'p99_latency > SLO_target',
      problem: 'Only fires when users are actually affected'
    },
    tradeoffs: [
      'CPU can be high with no user impact',
      'Latency directly measures user experience',
      'Many causes lead to latency; one alert catches all'
    ]
  },
  {
    causeBasedAlert: {
      name: 'Disk space low',
      condition: 'disk_free < 10%',
      problem: 'Fires on expected growth, archival systems, cold storage'
    },
    symptomBasedAlert: {
      name: 'Write failures increasing',
      condition: 'write_error_rate > 0.1%',
      problem: 'Only fires when disk space is actually causing failures'
    },
    tradeoffs: [
      'Low disk space is a potential future problem',
      'Write failures are a current user problem',
      'Keep disk space as a ticket, not page'
    ]
  },
  {
    causeBasedAlert: {
      name: 'Database connections high',
      condition: 'db_connections > 80% of max',
      problem: 'Connection pools often run hot without issues'
    },
    symptomBasedAlert: {
      name: 'Database query timeouts',
      condition: 'query_timeout_rate > 0.5%',
      problem: 'Only fires when connection exhaustion impacts queries'
    },
    tradeoffs: [
      'High connections may never cause problems',
      'Timeouts indicate actual user failures',
      'Connection monitoring becomes a dashboard, not alert'
    ]
  }
]
```

```
Mermaid diagram: Symptom-based alerting pyramid.
                    ┌─────────────────┐
                    │   PAGE ALERTS   │
                    │  (User Impact)  │
                    │ Error rate > X% │
                    │ Latency > SLO   │
                    └────────┬────────┘
                             │
              ┌──────────────┼──────────────┐
              │              │              │
     ┌────────▼───────┐ ┌────▼────┐ ┌───────▼───────┐
     │ TICKET ALERTS  │ │         │ │               │
     │ (Degradation)  │ │         │ │               │
     │ CPU sustained  │ │         │ │               │
     │ Disk filling   │ │         │ │               │
     └────────────────┘ └─────────┘ └───────────────┘
              │
     ┌────────▼────────┐
     │   DASHBOARDS    │
     │  (Awareness)    │
     │ All metrics     │
     │ Trends & rates  │
     └─────────────────┘

Pages: Wake someone up (user impact NOW)
Tickets: Fix during business hours (potential future impact)
Dashboards: Context for investigation (no action required)
```

### Deriving Symptoms from SLOs

Using SLO targets as alert thresholds.

```typescript
// SLO-derived alert configuration
interface SLODerivedAlert {
  slo: {
    name: string
    target: number
    window: string
  }
  burnRateAlert: {
    fast: { rate: number; window: string; severity: string }
    slow: { rate: number; window: string; severity: string }
  }
}

const sloAlerts: SLODerivedAlert[] = [
  {
    slo: {
      name: 'API Availability',
      target: 0.999,  // 99.9%
      window: '30d'
    },
    burnRateAlert: {
      // Fast burn: 14.4x burn rate exhausts budget in 2 hours
      fast: {
        rate: 14.4,
        window: '5m',
        severity: 'critical'
      },
      // Slow burn: 3x burn rate exhausts budget in 10 days
      slow: {
        rate: 3,
        window: '1h',
        severity: 'warning'
      }
    }
  }
]

// Prometheus alerting rules from SLO
const burnRateAlertRules = `
groups:
  - name: slo-burn-rate-alerts
    rules:
      # Fast burn alert (critical)
      - alert: APIAvailabilityFastBurn
        expr: |
          (
            sum(rate(http_requests_total{status=~"5.."}[5m]))
            / sum(rate(http_requests_total[5m]))
          ) > (14.4 * 0.001)
        for: 2m
        labels:
          severity: critical
          slo: api_availability
        annotations:
          summary: "API error rate burning SLO budget rapidly"
          description: "Current burn rate will exhaust monthly error budget in 2 hours"
          runbook_url: "https://runbooks.example.com/api-availability"

      # Slow burn alert (warning)
      - alert: APIAvailabilitySlowBurn
        expr: |
          (
            sum(rate(http_requests_total{status=~"5.."}[1h]))
            / sum(rate(http_requests_total[1h]))
          ) > (3 * 0.001)
        for: 15m
        labels:
          severity: warning
          slo: api_availability
        annotations:
          summary: "API error rate elevated"
          description: "Current burn rate will exhaust monthly error budget in 10 days"
          runbook_url: "https://runbooks.example.com/api-availability"
`
```

### Multi-Window Multi-Burn-Rate Alerts

More sophisticated alerting that balances speed and accuracy.

```typescript
// Multi-window burn rate configuration
interface MultiWindowBurnRate {
  shortWindow: string
  longWindow: string
  burnRate: number
  severity: 'critical' | 'warning' | 'info'
  budgetConsumedIn: string
}

const burnRateWindows: MultiWindowBurnRate[] = [
  // Page immediately: consuming budget very fast
  {
    shortWindow: '5m',
    longWindow: '1h',
    burnRate: 14.4,
    severity: 'critical',
    budgetConsumedIn: '2 hours'
  },
  // Page: significant burn rate sustained
  {
    shortWindow: '30m',
    longWindow: '6h',
    burnRate: 6,
    severity: 'critical',
    budgetConsumedIn: '5 hours'
  },
  // Ticket: moderate burn rate over longer period
  {
    shortWindow: '2h',
    longWindow: '24h',
    burnRate: 3,
    severity: 'warning',
    budgetConsumedIn: '10 days'
  },
  // Ticket: slow degradation
  {
    shortWindow: '6h',
    longWindow: '3d',
    burnRate: 1,
    severity: 'info',
    budgetConsumedIn: '30 days'
  }
]

// Why multi-window? Prevents both:
// - Short spikes triggering (need both windows to fire)
// - Long-resolved issues alerting (short window recovers)
```

<Callout type="info">
Requiring both a short AND long window to breach reduces false positives. A 1-minute spike won't trigger if the 1-hour window is healthy. A long-resolved issue won't keep alerting if the short window has recovered.
</Callout>

## Alert Signal Selection

Choosing the right metrics to base alerts on.

### Golden Signals as Alert Candidates

Using the four golden signals for symptom-based alerting.

```typescript
// Golden signals alert configuration
interface GoldenSignalAlerts {
  signal: 'latency' | 'traffic' | 'errors' | 'saturation'
  metrics: string[]
  alertOn: string
  dontAlertOn: string
  threshold: {
    page: string
    ticket: string
  }
}

const goldenSignalAlerts: GoldenSignalAlerts[] = [
  {
    signal: 'latency',
    metrics: ['http_request_duration_seconds', 'grpc_response_time'],
    alertOn: 'P99 latency exceeding SLO for X% of traffic',
    dontAlertOn: 'Average latency (hides tail issues)',
    threshold: {
      page: 'p99 > 2x SLO for > 5 minutes',
      ticket: 'p99 > 1.5x SLO for > 30 minutes'
    }
  },
  {
    signal: 'traffic',
    metrics: ['http_requests_total', 'messages_processed'],
    alertOn: 'Traffic drop > 50% from normal (possible outage)',
    dontAlertOn: 'High traffic (usually good, capacity is separate)',
    threshold: {
      page: 'Traffic < 50% of prediction for > 10 minutes',
      ticket: 'Traffic < 70% of prediction for > 1 hour'
    }
  },
  {
    signal: 'errors',
    metrics: ['http_requests_total{status=~"5.."}', 'error_count'],
    alertOn: 'Error rate exceeding error budget burn rate',
    dontAlertOn: 'Any single error (too noisy)',
    threshold: {
      page: 'Error rate > 14x budget burn rate',
      ticket: 'Error rate > 3x budget burn rate'
    }
  },
  {
    signal: 'saturation',
    metrics: ['cpu_usage', 'memory_usage', 'queue_depth'],
    alertOn: 'Resource causing user-visible impact',
    dontAlertOn: 'High resource usage alone',
    threshold: {
      page: 'When saturation causes latency/errors (symptom alert)',
      ticket: 'Saturation trending toward exhaustion'
    }
  }
]
```

### Absence of Data Alerts

Detecting when monitoring itself fails.

```typescript
// Absence detection patterns
interface AbsenceAlert {
  name: string
  condition: string
  falsePositiveRisk: string
  implementation: string
}

const absenceAlerts: AbsenceAlert[] = [
  {
    name: 'No requests received',
    condition: 'absent(rate(http_requests_total[5m]))',
    falsePositiveRisk: 'Legitimate low-traffic periods',
    implementation: `
      # Alert on absent data with time-of-day awareness
      - alert: NoRequestsReceived
        expr: |
          absent(rate(http_requests_total[5m]))
          and on() (hour() >= 6 and hour() <= 22)  # Business hours only
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "No requests received - possible collection failure"
    `
  },
  {
    name: 'Metric collection gap',
    condition: 'changes(up[5m]) == 0 and up == 1',
    falsePositiveRisk: 'Stable healthy systems',
    implementation: `
      # Detect stale metrics (no scrape updates)
      - alert: StaleMetrics
        expr: |
          time() - prometheus_target_scrape_last_success_timestamp_seconds > 120
        for: 3m
        labels:
          severity: warning
        annotations:
          summary: "Metrics stale for {{ $labels.instance }}"
    `
  },
  {
    name: 'Missing expected target',
    condition: 'absent(up{job="api-server"})',
    falsePositiveRisk: 'Intentional scale-down',
    implementation: `
      # Detect missing scrape targets
      - alert: TargetMissing
        expr: |
          absent(up{job="api-server"})
          unless on() deployment_replicas{deployment="api-server"} == 0
        for: 5m
        labels:
          severity: critical
    `
  }
]
```

## Threshold Tuning

Setting thresholds that balance sensitivity and noise.

### Data-Driven Threshold Selection

Using historical data to set appropriate thresholds.

```typescript
// Threshold analysis approach
interface ThresholdAnalysis {
  metric: string
  historicalAnalysis: {
    p50: number
    p95: number
    p99: number
    max: number
    incidentCorrelation: number[]  // Values during past incidents
  }
  recommendedThreshold: {
    value: number
    rationale: string
  }
}

// SQL query to analyze historical metric distribution
const thresholdAnalysisQuery = `
WITH metric_distribution AS (
  SELECT
    percentile_cont(0.50) WITHIN GROUP (ORDER BY value) as p50,
    percentile_cont(0.95) WITHIN GROUP (ORDER BY value) as p95,
    percentile_cont(0.99) WITHIN GROUP (ORDER BY value) as p99,
    max(value) as max_value
  FROM metrics
  WHERE metric_name = 'api_latency_p99'
    AND timestamp > NOW() - INTERVAL '30 days'
),
incident_values AS (
  SELECT DISTINCT m.value
  FROM metrics m
  JOIN incidents i ON m.timestamp BETWEEN i.start_time AND i.end_time
  WHERE m.metric_name = 'api_latency_p99'
    AND i.severity IN ('critical', 'major')
)
SELECT
  d.*,
  array_agg(iv.value) as incident_values,
  -- Recommended threshold: midpoint between p99 and incident minimum
  (d.p99 + min(iv.value)) / 2 as recommended_threshold
FROM metric_distribution d
CROSS JOIN incident_values iv
GROUP BY d.p50, d.p95, d.p99, d.max_value
`

// Threshold tuning guidelines
const thresholdGuidelines = {
  tooSensitive: {
    indicator: 'Alert fires multiple times per day without incident',
    action: 'Raise threshold or extend for duration'
  },
  tooInsensitive: {
    indicator: 'Incidents occur without alert firing first',
    action: 'Lower threshold or shorten for duration'
  },
  justRight: {
    indicator: '> 80% of alerts correlate with actual incidents',
    action: 'Document and monitor for drift'
  }
}
```

### Dynamic Thresholds

Adapting to changing baselines automatically.

```typescript
// Dynamic threshold calculation
interface DynamicThreshold {
  metric: string
  baselineWindow: string
  calculation: 'stddev' | 'percentile' | 'prophet'
  sensitivity: number
}

const dynamicThresholdExamples = {
  // Standard deviation based
  stddevBased: `
    # Alert when metric exceeds 3 standard deviations from moving average
    - alert: LatencyAnomaly
      expr: |
        (
          http_request_duration_seconds:p99:rate5m
          - http_request_duration_seconds:p99:rate5m:avg_over_time_1h
        )
        / http_request_duration_seconds:p99:rate5m:stddev_over_time_1h > 3
      for: 5m
  `,

  // Percentile based (robust to outliers)
  percentileBased: `
    # Alert when metric exceeds historical 99th percentile
    - alert: LatencyExceedsNormal
      expr: |
        http_request_duration_seconds:p99:rate5m
        > quantile_over_time(0.99, http_request_duration_seconds:p99:rate5m[7d])
      for: 10m
  `,

  // Time-aware baseline (accounts for daily/weekly patterns)
  timeAwareBased: `
    # Compare to same time last week
    - alert: TrafficAnomalyVsLastWeek
      expr: |
        abs(
          sum(rate(http_requests_total[5m]))
          - sum(rate(http_requests_total[5m] offset 7d))
        ) / sum(rate(http_requests_total[5m] offset 7d)) > 0.5
      for: 15m
  `
}
```

| Threshold Type | Best For | Weakness |
|---------------|----------|----------|
| Static | Stable metrics with known good values | Doesn't adapt to growth |
| Stddev-based | Metrics with normal distribution | Sensitive to outliers |
| Percentile-based | Metrics with skewed distribution | Slow to adapt to new baseline |
| Time-aware | Metrics with daily/weekly patterns | Requires sufficient history |
| ML-based | Complex patterns | Black box, hard to debug |

## Runbook Integration

Making alerts actionable with embedded remediation guidance.

### Runbook Structure

What every runbook should contain.

```typescript
// Runbook schema
interface Runbook {
  metadata: {
    alertName: string
    severity: 'critical' | 'warning' | 'info'
    lastUpdated: string
    owner: string
    reviewCycle: string
  }

  summary: {
    userImpact: string
    estimatedTimeToResolve: string
    escalationPath: string
  }

  diagnosis: {
    questions: string[]
    dashboardLinks: string[]
    logQueries: string[]
    commonCauses: { cause: string; likelihood: string; checkCommand: string }[]
  }

  remediation: {
    steps: { step: number; action: string; command?: string; expectedResult: string }[]
    rollbackProcedure: string
    verificationSteps: string[]
  }

  escalation: {
    whenToEscalate: string[]
    escalationContacts: { role: string; contact: string }[]
  }

  postIncident: {
    requiredActions: string[]
    preventionSuggestions: string[]
  }
}
```

```markdown
# Example Runbook: APIHighErrorRate

## Summary
- **User Impact**: Users experiencing 5xx errors on API requests
- **Time to Resolve**: 15-30 minutes typically
- **Escalation**: Page backend-oncall if not resolved in 30 min

## Diagnosis

### Key Questions
1. Is this affecting all endpoints or specific ones?
2. Did a deployment happen recently?
3. Are downstream dependencies healthy?

### Dashboard Links
- `[API Error Rate Dashboard](https://grafana/d/api-errors)`
- `[Dependency Health](https://grafana/d/dependencies)`
- `[Recent Deployments](https://deploy.internal/history)`

### Log Queries
```
service:api-gateway level:error | stats count by endpoint
```

### Common Causes
| Cause | Likelihood | Check |
|-------|------------|-------|
| Bad deployment | High | Check deploy history, rollback |
| Database overload | Medium | Check DB dashboard |
| Downstream timeout | Medium | Check dependency health |
| Memory pressure | Low | Check pod memory usage |

## Remediation

### Step 1: Identify Scope
```bash
kubectl logs -l app=api-gateway --tail=100 | grep -i error
```
**Expected**: Error pattern becomes clear

### Step 2: If Recent Deployment
```bash
kubectl rollout undo deployment/api-gateway
```
**Expected**: Error rate returns to normal within 2 minutes

### Step 3: If Database Issue
- Scale read replicas: `kubectl scale deployment/db-replica --replicas=5`
- Enable circuit breaker: Set `DB_CIRCUIT_BREAKER=true` in config

### Verification
- Error rate below 0.1% on `[dashboard](https://grafana/d/api-errors)`
- No new error logs in last 5 minutes
- User reports in #support channel resolved

## Escalation
- Page `backend-oncall` if: Not resolved in 30 min, error rate > 10%, affecting payments
- Page `database-oncall` if: Database-related and DBA intervention needed
- Page `incident-commander` if: Customer-facing outage > 1 hour

## Post-Incident
- [ ] Create incident ticket
- [ ] Update this runbook if new cause discovered
- [ ] Schedule post-mortem if P1/P2
```

### Alert-Runbook Linking

Ensuring every alert points to its runbook.

```typescript
// Alert with embedded runbook link
const alertWithRunbook = `
groups:
  - name: api-alerts
    rules:
      - alert: APIHighErrorRate
        expr: |
          sum(rate(http_requests_total{status=~"5.."}[5m]))
          / sum(rate(http_requests_total[5m])) > 0.01
        for: 5m
        labels:
          severity: critical
          team: backend
          runbook: "api-high-error-rate"
        annotations:
          summary: "API error rate is {{ $value | humanizePercentage }}"
          description: |
            The API error rate has exceeded 1% for the last 5 minutes.
            Current error rate: {{ $value | humanizePercentage }}

          runbook_url: "https://runbooks.example.com/api-high-error-rate"

          dashboard_url: "https://grafana.example.com/d/api-errors?var-window=5m"

          log_query: |
            {service="api-gateway"} |= "error" | json | line_format "{{.level}} {{.message}}"
`

// Runbook validation in CI
interface RunbookValidation {
  alertName: string
  hasRunbook: boolean
  runbookUrl: string
  lastUpdated: string
  daysSinceUpdate: number
  validationErrors: string[]
}

async function validateAlertRunbooks(alertRules: AlertRule[]): Promise<RunbookValidation[]> {
  const results: RunbookValidation[] = []

  for (const alert of alertRules) {
    const runbookUrl = alert.annotations?.runbook_url

    const validation: RunbookValidation = {
      alertName: alert.alert,
      hasRunbook: !!runbookUrl,
      runbookUrl: runbookUrl || 'MISSING',
      lastUpdated: '',
      daysSinceUpdate: 0,
      validationErrors: []
    }

    if (!runbookUrl) {
      validation.validationErrors.push('Missing runbook_url annotation')
    } else {
      // Verify runbook exists and is current
      const runbook = await fetchRunbook(runbookUrl)
      if (!runbook) {
        validation.validationErrors.push('Runbook URL returns 404')
      } else {
        validation.lastUpdated = runbook.metadata.lastUpdated
        validation.daysSinceUpdate = daysSince(runbook.metadata.lastUpdated)

        if (validation.daysSinceUpdate > 90) {
          validation.validationErrors.push('Runbook not updated in 90+ days')
        }
      }
    }

    results.push(validation)
  }

  return results
}
```

<Callout type="success">
No alert can be merged to production without a corresponding runbook. CI checks validate that `runbook_url` annotation exists and points to a valid document. Stale runbooks (> 90 days) trigger review tickets.
</Callout>

## Alert Routing and Escalation

Getting alerts to the right people at the right time.

### Routing Configuration

Directing alerts based on severity, team, and time.

```yaml
# Alertmanager routing configuration
route:
  receiver: 'default-receiver'
  group_by: ['alertname', 'service']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 4h

  routes:
    # Critical alerts: page immediately
    - match:
        severity: critical
      receiver: 'pagerduty-critical'
      group_wait: 0s
      repeat_interval: 15m

    # Warning alerts during business hours: Slack
    - match:
        severity: warning
      receiver: 'slack-warnings'
      active_time_intervals:
        - business-hours

    # Warning alerts outside business hours: ticket only
    - match:
        severity: warning
      receiver: 'ticket-system'
      active_time_intervals:
        - outside-business-hours

    # Team-specific routing
    - match:
        team: payments
      receiver: 'payments-team'
      routes:
        - match:
            severity: critical
          receiver: 'payments-pagerduty'

receivers:
  - name: 'pagerduty-critical'
    pagerduty_configs:
      - service_key: '<pagerduty-key>'
        severity: critical
        description: '{{ .CommonAnnotations.summary }}'
        details:
          runbook: '{{ .CommonAnnotations.runbook_url }}'
          dashboard: '{{ .CommonAnnotations.dashboard_url }}'

  - name: 'slack-warnings'
    slack_configs:
      - api_url: '<slack-webhook>'
        channel: '#alerts-warnings'
        title: '{{ .CommonAnnotations.summary }}'
        text: |
          *Runbook*: {{ .CommonAnnotations.runbook_url }}
          *Dashboard*: {{ .CommonAnnotations.dashboard_url }}

time_intervals:
  - name: business-hours
    time_intervals:
      - weekdays: ['monday:friday']
        times:
          - start_time: '09:00'
            end_time: '18:00'
        location: 'America/New_York'

  - name: outside-business-hours
    time_intervals:
      - weekdays: ['monday:friday']
        times:
          - start_time: '00:00'
            end_time: '09:00'
          - start_time: '18:00'
            end_time: '24:00'
      - weekdays: ['saturday', 'sunday']
```

### Escalation Policies

Automatic escalation when alerts aren't acknowledged.

```typescript
// Escalation policy configuration
interface EscalationPolicy {
  name: string
  description: string
  levels: EscalationLevel[]
}

interface EscalationLevel {
  order: number
  delayMinutes: number
  targets: EscalationTarget[]
}

interface EscalationTarget {
  type: 'schedule' | 'user' | 'team'
  id: string
}

const criticalEscalationPolicy: EscalationPolicy = {
  name: 'Critical Production Issues',
  description: 'For P1 production incidents',
  levels: [
    {
      order: 1,
      delayMinutes: 0,
      targets: [{ type: 'schedule', id: 'primary-oncall' }]
    },
    {
      order: 2,
      delayMinutes: 10,
      targets: [
        { type: 'schedule', id: 'secondary-oncall' },
        { type: 'user', id: 'engineering-manager' }
      ]
    },
    {
      order: 3,
      delayMinutes: 20,
      targets: [
        { type: 'team', id: 'engineering-leadership' },
        { type: 'user', id: 'vp-engineering' }
      ]
    },
    {
      order: 4,
      delayMinutes: 30,
      targets: [
        { type: 'user', id: 'cto' }
      ]
    }
  ]
}

// Escalation tracking
interface EscalationState {
  alertId: string
  currentLevel: number
  acknowledgedBy?: string
  acknowledgedAt?: Date
  escalationHistory: {
    level: number
    notifiedAt: Date
    notifiedTargets: string[]
  }[]
}
```

```
Mermaid diagram: Escalation timeline.
Alert Fires (T+0)
     ↓
Page Primary On-Call
     │
     ├── Acknowledged? → Escalation stops
     │
     ↓ (T+10 min, no ack)
Page Secondary + Manager
     │
     ├── Acknowledged? → Escalation stops
     │
     ↓ (T+20 min, no ack)
Page Leadership Team
     │
     ├── Acknowledged? → Escalation stops
     │
     ↓ (T+30 min, no ack)
Page CTO (final level)

Auto-resolve after 1 hour if metric recovers
```

## Alert Hygiene and Maintenance

Keeping alerting systems effective over time.

### Regular Alert Reviews

Scheduled assessment of alert effectiveness.

```typescript
// Alert review checklist
interface AlertReviewChecklist {
  alert: string
  reviewDate: string
  reviewedBy: string
  checks: {
    check: string
    result: 'pass' | 'fail' | 'needs_improvement'
    notes: string
  }[]
  decision: 'keep' | 'modify' | 'delete' | 'convert_to_ticket'
}

const alertReviewTemplate: AlertReviewChecklist = {
  alert: '',
  reviewDate: '',
  reviewedBy: '',
  checks: [
    {
      check: 'Has the alert fired in the last 30 days?',
      result: 'pass',
      notes: 'Fired 3 times, all actionable'
    },
    {
      check: 'Were all firings actionable (required human intervention)?',
      result: 'pass',
      notes: '3/3 required rollback'
    },
    {
      check: 'Is the runbook current and accurate?',
      result: 'needs_improvement',
      notes: 'Step 3 references deprecated tool'
    },
    {
      check: 'Is the threshold still appropriate?',
      result: 'pass',
      notes: 'Aligns with current SLO'
    },
    {
      check: 'Should this be a page or a ticket?',
      result: 'pass',
      notes: 'Correctly configured as page'
    },
    {
      check: 'Are there duplicate or overlapping alerts?',
      result: 'fail',
      notes: 'Similar alert exists in other-service'
    }
  ],
  decision: 'modify'
}

// Alert review meeting agenda
const alertReviewAgenda = `
## Monthly Alert Review - [Date]

### Metrics Review (10 min)
- Total alerts: ___
- Actionable rate: ___%
- False positive rate: ___%
- Top 5 noisy alerts: ___

### Individual Alert Review (30 min)
For each alert that fired:
1. Was it actionable?
2. Was the runbook helpful?
3. Should threshold change?
4. Keep/modify/delete?

### New Alert Proposals (10 min)
- Review proposed new alerts
- Ensure runbook exists
- Verify symptom-based design

### Action Items (10 min)
- Assign alert modifications
- Schedule runbook updates
- Plan threshold adjustments
`
```

### Alert Lifecycle Management

From creation to retirement.

| Phase | Activities | Owner |
|-------|------------|-------|
| Proposal | RFC with justification, runbook draft, threshold analysis | Developer |
| Review | Team review for symptom-based design, noise assessment | Tech Lead |
| Testing | Deploy to staging, validate fires correctly | Developer |
| Deployment | Merge alert rule, publish runbook | Developer |
| Monitoring | Track actionable rate, false positives | On-call team |
| Tuning | Adjust thresholds based on data | Alert owner |
| Review | Quarterly assessment, update or retire | Team |
| Retirement | Remove alert, archive runbook | Alert owner |

<Callout type="warning">
An alert that hasn't fired in 6 months or has < 50% actionable rate should be deleted or converted to a dashboard. Unused alerts create maintenance burden and false confidence. Be aggressive about pruning.
</Callout>

## Conclusion

Alert fatigue is a design problem, not a discipline problem. Symptom-based alerting focuses on user impact—latency, errors, availability—rather than causes like CPU or disk space. Derive thresholds from SLOs using multi-window burn rates that balance speed of detection with noise reduction. Every alert must have a runbook that answers: what's the impact, how do I diagnose, how do I fix it, when do I escalate? Route alerts appropriately: pages for user-impacting issues requiring immediate action, tickets for degradation that can wait until business hours, dashboards for awareness. Run monthly alert reviews to prune noise, update runbooks, and adjust thresholds based on real data. The goal is not zero alerts—it's ensuring every alert that fires represents a real problem that requires human intervention, and the human receiving it has everything they need to resolve it quickly.
